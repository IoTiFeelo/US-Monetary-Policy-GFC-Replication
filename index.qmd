---
title: "A Proxy-SVAR Reproduction Model"
author: "Carbonara without Cream"
execute:
  echo: false
bibliography: references.bib
editor: 
  markdown: 
    wrap: 72
---

```{css, echo = FALSE}
.justify {
text-align: justify !important
}
```

::: justify
> **Abstract.**
>
> **Keywords.** proxy svars, impulse responses, U.S. mps

# Introduction

I replicate @miranda2020us, Review of Economic Studies.
In their paper, the authors jointly evaluate the effects of financial,
monetary and real variables, in the U.S. and abroad, following a 1%
shock of the Federal Reserve (FED) interest rate. In particular, the
authors rely on an instrumental variable to identify U.S. monetary
policy shocks. This is to avoid implausible restrictions on their
variable of interest. My task will be to understand the model of these
two scholars and replicate it with simplifications given the time scope
of the Macroeconometrics course taught by Dr. [Tomasz
Wo≈∫niak](https://findanexpert.unimelb.edu.au/profile/426516-tomasz-wozniak).
The specific challenge will be writing R codes to disentangle their
framework.

# The Paper

## Global Factor in Risky Asset Prices

[Miranda-Agrippino and
Rey](http://helenerey.eu/Content/_Documents/MirandaAgrippinoRey_REStud_Final.pdf)
in the first part of the paper estimate a global factor to proxy the
movement of world risky asset prices. They do so by collecting 858
prices of different risky assets traded in North America, Latin America,
Europe, Asia Pacific, and Australia, from 1990 to 2012. Their method is
to pick a representative market index (i.e. S&P 500) for each market at
the end of 2012, including all of its components, selecting prices that
allow them to cover at least 80% of cross sectional observations by 1990
and 95% in 1995. They do so to avoid over-representation of each
category. With this global factor, they can explain over 20% of global
risky asset price volatility in their time span. Given the small time
frame and VAR analysis limitations, they estimate a global factor with
commodities from the U.S., Europe, and Japan, spanning back to 1975.
This factor covers 60% of the volatility in this period. The appendix of
the paper provides detailed information on this VAR estimation. To
provide more intuition on this factor, the authors correlate it with
some indexes of implied volatility such as the VIX, outlining its
co-movement with common measures of market variation (in this case a
negative correlation). The global factor will be used later in the
impulse-response section.

```{r package load}
#| echo: false
#set working directory 
#setwd("/Users/filo/Dropbox/Macrometrics/US-Monetary-Policy-and-the-Global-Financial-Cycle-Replication")
##########################################################################################
library(readxl)
library(ggplot2)
library(rmarkdown)
library(matrixcalc)
library(mvtnorm)
library(parallel)
library(fredr)
library(sovereign)
library(MASS)
##########################################################################################

url <- "http://silviamirandaagrippino.com/s/DFM-Blocks.zip"
download.file(url, "MAR(2020)")
unzip("MAR(2020)")
data <- read_excel("GFC_VARdata+WEB.xlsx", skip=1, .name_repair = "unique_quiet")
inst <- read_excel("GFC_VARdata+WEB.xlsx", sheet = 2, skip=1, .name_repair = "unique_quiet")
#date = ts(seq(from = as.Date("1980-01-01"), to = as.Date("2012-12-01"), by = 'month'))
#instrument$LABEL <- date #ts object is not able to translate dates
gf <- read_excel("GFC_VARdata+WEB.xlsx", sheet= 4, skip=2, .name_repair = "unique_quiet", na = "blank")
#monthly data for monthly FedFundsRate
fed <- (fredr(series_id = "FEDFUNDS", 
                         observation_start = as.Date("1975-01-01"), 
                         frequency = "m",
                         aggregation_method = "average"))

##########################################################################################
ggplot() + 
  geom_line(data = gf, aes(x = `...1`, y = `GLOBAL FACTOR 1975-2010`, linetype = "solid"), color ="blue", na.rm = TRUE) + 
  geom_line(data = gf, aes(x = `...1`, y = `GLOBAL FACTOR 1990-2012`, linetype = "twodash"), color="blue4", na.rm = TRUE) +
  geom_line(data = data, aes(x = `LABEL`, y = VIX, linetype = "dotted"), color="black", na.rm = TRUE) +
  scale_linetype_manual(values=c("solid", "twodash", "dotted"), name="Global Factors", 
                        labels=c("VIX", "GF 1975-2010", "GF 1990-2012")) +
  guides(linetype = guide_legend(override.aes = list(color = c("black", "blue", "blue4")))) +
  labs(x ="", y="", title= "Global Factor for Risky Asset Prices") +
  theme(legend.position = "bottom", plot.title = element_text(hjust = 0.5))

```

## Proxy-VAR Analysis with Rich-Information Bayesian VAR

In this project, I will avoid the computation of the global factor,
because it is out of scope of the Macroeconomerics subject. Instead, I
will concentrate on the Bayesian VAR analysis of Miranda-Agrippino and
Rey. A main reason why the authors studied the monetary effects of U.S.
interest rate changes is that the dollar is the currency of global
banking. A change in FED monetary policy affects banks' borrowing
capacity, the pricing of dollar denominated assets, and cross-border
capital flows. In order to isolate its effects, the two scholars
identify U.S. monetary policy shocks by exploiting 30-min price
revisions around Federal Open Market Committee
([FOMC](https://www.federalreserve.gov/monetarypolicy/fomc.htm))
announcements in the fourth federal funds futures contracts
([FF4](https://www.investopedia.com/terms/f/fed-funds-futures.asp)). The
intuition is that these
[futures](https://www.investopedia.com/terms/f/futures.asp) have an
average maturity of three months, and they can predict revisions of
market expectations about future monetary policy one-quarter in advance.
This assumption holds only if market participants can distinguish
between the systematic component of policy and any observable policy
action. Moreover, with asymmetrical information, the FF4 revisions
contain information about the influence of economic factors relevant to
U.S. monetary policy. Policy announcements provide this information
implicitly.

## The Data

I download the data directly from the website of Miranda-Agrippino. The
two authors study the consequences of a 1% increase in the U.S. monetary
policy considering:

-a domestic VAR with the effects on domestic financial markets and
macroeconomic aggregates in the United States;

-a global VAR with the effects on global asset markets, global domestic
credit and international capital flows;

-a "floaters" VAR to study if a fixed or pegged exchange rate affects
the global contraction.

I will study the global specifications, and I will include the following
variables:

```{r package load2}
library(ggplot2)
##########
ggplot(data=data) + 
  geom_line(aes(x = `LABEL`, y = GLBCREDIT, linetype = "solid"), na.rm = TRUE, color = "blue") + geom_line(aes(x = `LABEL`, y = GLBCREXUS, linetype = "dashed"), na.rm = TRUE, color = "black") + geom_line(aes(x = `LABEL`, y = NONREVSL, linetype = "twodash"), na.rm = TRUE, color = "grey") +
  scale_linetype_manual(values=c("solid", "dashed", "twodash"), name="Global Credit Types", labels=c("Global Domestic Credit Ex US", "Global Domestic Credit", "US Total Non-revolving Credit"), guide = guide_legend(override.aes = list(color = c("blue", "black", "grey")))) +
  guides(linetype = guide_legend(override.aes = list(color = c("black", "blue", "grey")))) +
  labs(x ="", y="", title= "Global Credit") +
  theme(legend.position = "bottom", plot.title = element_text(hjust = 0.5, size = 5), axis.text = element_text(size = 3))

#######
ggplot(data=data) + 
  geom_line(aes(x = `LABEL`, y = GLBINFLALL, linetype = "solid"), na.rm = TRUE, color = "blue") + geom_line(aes(x = `LABEL`, y = GLBINFLB, linetype = "dashed"), na.rm = TRUE, color = "black") + geom_line(aes(x = `LABEL`, y = GLBINFLNB, linetype = "twodash"), na.rm = TRUE, color = "grey") +
  scale_linetype_manual(values=c("solid", "dashed", "twodash"), name="Global Inflow Types", labels=c("Global Inflows to Banks", "Global Inflows All Sectors", "Global Inflows to Non-Banks"), guide = guide_legend(override.aes = list(color = c("blue", "black", "grey")))) +
  guides(linetype = guide_legend(override.aes = list(color = c("black", "blue", "grey")))) +
  labs(x ="", y="", title= "Global Inflows") +
  theme(legend.position = "bottom", plot.title = element_text(hjust = 0.5))

########
ggplot(data=data) + 
  geom_line(aes(x = `LABEL`, y = EUBDLEV, linetype = "solid"), na.rm = TRUE, color = "blue") + geom_line(aes(x = `LABEL`, y = USBANKSL, linetype = "dashed"), na.rm = TRUE, color = "black") + geom_line(aes(x = `LABEL`, y = EUBANKSL, linetype = "twodash"), na.rm = TRUE, color = "grey") +
  scale_linetype_manual(values=c("solid", "dashed", "twodash"), name="Leverage Types", labels=c("Leverage US Banks", "Leverage EU Global Banks", "Leverage EU Banks"), guide = guide_legend(override.aes = list(color = c("blue", "black", "grey")))) +
  guides(linetype = guide_legend(override.aes = list(color = c("black", "blue", "grey")))) +
  labs(x ="", y="", title= "Leverage") +
  theme(legend.position = "bottom", plot.title = element_text(hjust = 0.5))

##########
ggplot(data=data) + 
  geom_line(aes(x = `LABEL`, y = GREA, linetype = "solid"), na.rm = TRUE, color = "blue") + geom_line(aes(x = `LABEL`, y = GREAEXUS, linetype = "dashed"), na.rm = TRUE, color = "black") + geom_line(aes(x = `LABEL`, y = INDPRO, linetype = "twodash"), na.rm = TRUE, color = "grey") +
  scale_linetype_manual(values=c("solid", "dashed", "twodash"), name="Production Types", labels=c("Global Real Economic Activity Ex US", "Global Real Economic Activity", "Industrial Production"), guide = guide_legend(override.aes = list(color = c("black", "blue", "grey")))) +
  labs(x ="", y="", title= "Production") +
  theme(legend.position = "bottom", plot.title = element_text(hjust = 0.5))

###############
ggplot(data=data) + 
  geom_line(aes(x = `LABEL`, y = GLOBALF, linetype = "solid"), na.rm = TRUE, color = "blue") + geom_line(aes(x = `LABEL`, y = GLOBALRA, linetype = "dashed"), na.rm = TRUE, color = "black") +
  scale_linetype_manual(values=c("solid", "dashed"), name="Risk Types", labels=c("Global Risk Aversion", "Global Factor"), guide = guide_legend(override.aes = list(color = c("black", "blue")))) +
  labs(x ="", y="", title= "Risk Aversion") +
  theme(legend.position = "bottom", plot.title = element_text(hjust = 0.5))
```

# The Model

## BVAR Framework

From Herwatz, Rohloff & Wang(2022) I define the model as:
\begin{gather} \label{eq:solve}
y_t=\gamma+A_1y_{t-1}+\dots+A_py_{t-p}+u_t \\
y_t=\gamma+A_1y_{t-1}+\dots+A_py_{t-p}+B\varepsilon_t,&\:\:\:\:\: t={1,
\dots ,T} \end{gather} where $A_j, : \left \{j=1,2,\dots,p\right \}$ are
a K x K coefficient matrices, and $u_t$ in the first reduced form is
serially uncorrelated with zero mean and positive definite
(non-diagonal) covariance matrix $\Sigma_u.$ The structural shocks
$\varepsilon_t$ in the second reduced form assumed to be mutually
uncorrelated and normalised to have unit variance.
$\Xi\left(\varepsilon_t \varepsilon_t' \right)=I_K$. Structural shocks
are mapped to the reduced-form system through a K x K non-singular
matrix $B$, such that $BB^{'}=\Sigma_u$. For simplicity, the process is
assumed to be causal and $detA(z)=det(I_k-\sum_{j=1}^{p}A_jz^{j})\neq 0$
for $\left| z \right|\le 1$. This ensures that the process has a Wold
moving average MA representation. Moreover,

```{=tex}
\begin{align*}
y_t=\mu+\sum_{i=0}^{\infty}\Phi_iu_{t-i}=\mu+\sum_{i=0}^{\infty}\Phi_iB\varepsilon_{t-i}=
\mu+\sum_{i=0}^{\infty}\Theta_i\varepsilon_{t-i}.
\end{align*}
```
With: \begin{gather}
\mu=A(1)^{-1}\nu, ::: \Phi_0=I_K, \ \Phi_i=\sum_{j=1}^{i}A_j\Phi_{i-j}, ::: A_j=0 :for: j>p
\end{gather}. The second to last MA representation is of particular
importance because the structural MA coefficients $\Theta_i=\Phi_iB$
cannot be covered without a proper identification. I will briefly
outline the Proxy SVAR approach.

Let $z_t$ be an external instrument to identify the structural shock of
interest $\varepsilon_kt, k\:\epsilon \: \left \{1,\dots, K\right \}$.
$z_t$ has to satisfy the *relevant* condition
$\Xi(\varepsilon_{kt}z_t)=\phi \neq 0$ and the *exogeneity* condition
$\Xi(\varepsilon{lt}z_t)=0, \forall l\:\epsilon\left \{1,\dots, K\right \}\setminus\left \{k\right \}$.

# My Coding Interpretation

## Basic Model

```{r package load3}
#| echo: false
########################################################################################################################
  fed        = ts((fed[,3]), start=c(1980,1), frequency=12) #FED federal funds rate
  instrument = ts((inst$FF4), start=c(1980,1), frequency=12) #FFF4 instrument
  pce        = ts((data$PCEPI), start=c(1980,1), frequency=12) #PCE Deflator; 
  #dgs1       = ts((data$DGS1), start=c(1980,1), frequency=12) #1 Year Treasury Rate
  #bis        = ts((data$BISREER), start=c(1980,1), frequency=12) #BIS real EER
  globalf    = ts((data$GLOBALF), start=c(1980,1), frequency=12) #Global Factor
  #globalra   = ts((data$GLOBALRA), start=c(1980,1), frequency=12) #Global Risk Aversion
  greaexus   = ts((data$GREAEXUS), start=c(1980,1), frequency=12) #Global Real Economic Activity Ex US
  indpro     = ts((data$INDPRO), start=c(1980,1), frequency=12) #industrial production
  #glbcredit    = ts((data$GLBCREDIT), start=c(1980,1), frequency=12) #Global Domestic Credit
  glbinflows    = ts((data$GLBINFLALL), start=c(1980,1), frequency=12) #Global Inflows All Sectors
  #usbdlev    = ts((data$USBDLEV), start=c(1980,1), frequency=12) #Leverage US Brokers and Dealers
  eubdlev    = ts((data$EUBDLEV), start=c(1980,1), frequency=12) #Leverage EU Global Banks
  #usbanksl   = ts((data$USBANKSL), start=c(1980,1), frequency=12) #Leverage US Banks
  #eubanksl   = ts((data$EUBANKSL), start=c(1980,1), frequency=12) #Leverage EU Banks

  #create the bigy matrix with my data
  y = cbind(instrument,
    fed,
    pce, 
    globalf, 
    greaexus, 
    indpro,
    glbinflows, 
    eubdlev 
  )
  
  # setup function for my analysis
  basic.model <- function(bigy, p, start, end){
  bigy <- window(bigy, start=start, end=end)
  S=5
  kappa.3=0.95
  ############################################################
  N       = ncol(bigy)
  K       = 1+N*p
  S.burnin= 100
  ############################################################
  Y       = bigy[(p+1):nrow(bigy),]
  X       = matrix(1,nrow(Y),1)
  for (i in 1:p){
    X     = cbind(X,bigy[(p+1):nrow(bigy)-i,])
  }
  # set the priors
  ############################################################
  A.prior     = matrix(0,K,N)
  A.prior[2:(N+1),] <- kappa.3*diag(N)
  V.prior     = (diag(c(10,1*((1:p)^(-2))%x%rep(1,N)))) #10 is kappa.2, 1 is kappa.1
  S.prior     = diag(N)
  nu.prior    = N+1
  
  # normal-inverse Wishart posterior parameters
  ############################################################
  V.bar.inv   = t(X)%*%X + diag(1/diag(V.prior)) #X'X+diag(V^-1)
  V.bar       = solve(V.bar.inv) #inv(X'X+diag(V^-1))
  A.bar       = V.bar%*%(t(X)%*%Y + diag(1/diag(V.prior))%*%A.prior) #V.bar(X'X+diag((V.prior)^-1(A.prior)))
  nu.bar      = nrow(Y) + nu.prior 
  S.bar       = S.prior + t(Y)%*%Y + t(A.prior)%*%diag(1/diag(V.prior))%*%A.prior - t(A.bar)%*%V.bar.inv%*%A.bar
  #S.prior+Y'Y+(A.prior)'*diag(diag((V.prior)^-1))*A.prior-A.bar'*(X'X+diag(V^-1))*A.bar
  S.bar.inv   = solve(S.bar)
  
  # posterior draws
  ############################################################
  Sigma.posterior   = rWishart(S, df=nu.bar, Sigma=S.bar.inv)
  Sigma.posterior   = apply(Sigma.posterior,3,solve)
  Sigma.posterior   = array(Sigma.posterior,c(N,N,S))
  A.posterior       = array(rnorm(prod(c(dim(A.bar),S))),c(dim(A.bar),S)) #3 dimensional, S repetitions of rnorm
  B.posterior       = array(NA,c(N,N,S))
  L                 = t(chol(V.bar))
  for (s in 1:S){
    cholSigma.s     = chol(Sigma.posterior[,,s])
    B.posterior[,,s]= t(cholSigma.s)
    A.posterior[,,s]= A.bar + L%*%A.posterior[,,s]%*%cholSigma.s
}
  results = list("B" = B.posterior, "A" = A.posterior )
  return(results)
  }
  
  #run the function with my data
  set.seed(23)
  basic.results <- basic.model(y, 12, c(1990,2), c(2010,12))

```

##Extended Model

```#{r package load6}
  extended.model <- function(bigy, p, S1, S2, kappa.3, start, end){
  #bigy <- window(bigy, start=start, end=end)
   y <- window(y, start=c(1990,2), end=c(2010,12))
   bigy=y
   p=12
   S1=5 #repetitions to discard
   S2=5 #repetitions to keep
   S=S1+S2
   kappa.1=1
   kappa.2=10
   kappa.3=0.95
  ############################################################
  
  Y       = bigy[(p+1):nrow(bigy),]
  X       = matrix(1,nrow(Y),1)
  for (i in 1:p){
    X     = cbind(X,bigy[(p+1):nrow(bigy)-i,])
  }
  
  N       = ncol(bigy)
  K       = 1+N*p
  S.burnin= 100
  # set the priors
  ############################################################
  # priors <- setpriors(N,K,kappa.3)
  # priors$A.prior  = A.prior
  # priors$V.prior  = V.prior
  # priors$S.prior  = S.prior
  A.prior     = matrix(0,K,N)
  A.prior[2:(N+1),] <- kappa.3*diag(N)
  V.prior     = (diag(c(10,1*((1:p)^(-2))%x%rep(1,N)))) #10 is kappa.2, 1 is kappa.1
  S.prior     = diag(N)
  nu.prior    = N+1
  s.sigma.prior.k = diag(N)   #for kappa.sigma and kappa.a
  nu.prior.k      = N + 1      #same, to be used in ig2 and rgamma
  #priors$nu.prior = nu.prior
  s.kappa         = 2
  s.sigma.prior   = 2
  a.sigma.prior   = 2
  
  k.sigma = matrix(NA, S, 1)
  k.a     = matrix(NA, S, 1)
  
  k.sigma[1] <- 1
  k.a[1] <- 1
  for (s in 1:5) {
  # normal-inverse Wishart, IG2, and Gamma posterior parameters
  ############################################################
  V.bar.inv   = t(X)%*%X + solve(k.a[s]*V.prior)
  V.bar       = solve(V.bar.inv)
  A.bar       = V.bar%*%(t(X)%*%Y + diag(1/diag(k.a[s]*V.prior))%*%A.prior)
  nu.bar      = nrow(Y) + nu.prior
  nu.bar.k    = nu.prior.k + N*K
  S.bar       = k.sigma[s]*diag(N) + t(Y)%*%Y + t(A.prior)%*%diag(1/diag(k.a[s]*V.prior))%*%A.prior - t(A.bar)%*%V.bar.inv%*%A.bar
  S.bar.inv   = solve(S.bar)
  a.sigma.bar = (N*V.prior)/2+a.sigma.prior
  
  # posterior draws
  ############################################################
  Sigma.posterior     = rWishart(S, df=nu.bar, Sigma=S.bar.inv)
  Sigma.posterior     = apply(Sigma.posterior,3,solve)
  Sigma.posterior     = array(Sigma.posterior,c(N,N,S))
  sigma.post.temp     = Sigma.posterior[,,s]
  A.posterior         = array(rnorm(prod(c(dim(A.bar),S))),c(dim(A.bar),S))
  B.posterior         = array(NA,c(N,N,S))
  Bplus.posterior     = array(NA,c(N,K,S))
  L                   = t(chol(V.bar))
    cholSigma.s       = chol(Sigma.posterior[,,s])
    B.posterior[,,s]  = t(cholSigma.s)
    A.posterior[,,s]  = A.bar + L%*%A.posterior[,,s]%*%cholSigma.s
    #k.a from IG2
    s.k.a.bar     <- s.kappa + sum(diag( sigma.post.temp %*% t(A.posterior[,,s]-A.prior)%*% diag(1/diag(V.prior)) %*% (A.posterior[,,s]-A.prior)))
    k.a[s+1]      <- s.k.a.bar / rchisq(1, df=nu.bar.k)
    #k.sigma from gamma distribution
    s.sigma.bar   <- (2 + sum(diag(sigma.post.temp*s.sigma.prior))^-1+(s.sigma.prior)^-1)^-1
    k.sigma[s+1]  <- rgamma(1, shape = a.sigma.bar, scale = s.sigma.bar)
    
    
  }
  
  results = list("B" = B.posterior[,,10], "A" = A.posterior[,,10], "k.a" = k.a, "k.sigma" = k.sigma)
  return(results)
  }
  
  y                   <- window(y, start=c(1990,2), end=c(2010,12))
  extended.result     <- extended.model(y, 12, 5, 5, kappa.3=0.95, start=c(1990,2), end=c(2010,12))

```

## Artificial Data

```{r package load6}
  #artificial data with 1000 observations from a bi-variate Gaussian random walk
  #VAR model with N=2, p=1, and a constant term
  set.seed(23)   # set seed for reproducibility
  # simulate 1000 samples from the bivariate Gaussian random walk process with cumsum(rnorm())
  y1 <- ts(cumsum(rnorm(1000, 0, sd=1)))
  y2 <- ts(cumsum(rnorm(1000, 0, sd=1)))
  y.art <- cbind(y1,y2)
  plot(y.art)
  # run the function of the basic model and check the posterior parameters B0 and B1
  basic.artificial <- basic.model(y.art, 1, c(), c())
```

# References
:::

---
title: "A Proxy-SVAR Reproduction Model"
author: "Carbonara without Cream"
execute:
  echo: false
bibliography: references.bib
editor: 
  markdown: 
    wrap: 72
---

```{css, echo = FALSE}
.justify {
text-align: justify !important
}
```

::: justify
**Keywords.** proxy svars, impulse responses, U.S. mps

# Introduction

We replicate @miranda2020us, Review of Economic Studies. In their paper,
the authors jointly evaluate the effects of financial, monetary and real
variables, in the U.S. and abroad, following a 1% shock of the Federal
Reserve (FED) interest rate. In particular, the authors rely on an
instrumental variable to identify U.S. monetary policy shocks. This is
to avoid implausible restrictions on their variable of interest. My task
will be to understand the model of these two scholars and replicate it
with simplifications given the time scope of the Macroeconometrics
course taught by Dr. [Tomasz
Woźniak](https://findanexpert.unimelb.edu.au/profile/426516-tomasz-wozniak).
The specific challenge will be writing R codes to disentangle their
framework. We replicate @miranda2020us, Review of Economic Studies. In
their paper, the authors jointly evaluate the effects of financial,
monetary and real variables, in the U.S. and abroad, following a 1%
shock of the Federal Reserve (FED) interest rate. In particular, the
authors rely on an instrumental variable to identify U.S. monetary
policy shocks. This is to avoid implausible restrictions on their
variable of interest. Our task will be to understand the model of these
two scholars and replicate it with simplifications given the time scope
of the Macroeconometrics course taught by Dr. [Tomasz
Woźniak](https://findanexpert.unimelb.edu.au/profile/426516-tomasz-wozniak).
The specific challenge will be writing R codes to disentangle their
framework.

# The Paper

## Global Factor in Risky Asset Prices

[Miranda-Agrippino and
Rey](http://helenerey.eu/Content/_Documents/MirandaAgrippinoRey_REStud_Final.pdf)
in the first part of the paper estimate a global factor to proxy the
movement of world risky asset prices. They do so by collecting 858
prices of different risky assets traded in North America, Latin America,
Europe, Asia Pacific, and Australia, from 1990 to 2012. Their method is
to pick a representative market index (i.e. S&P 500) for each market at
the end of 2012, including all of its components, selecting prices that
allow them to cover at least 80% of cross sectional observations by 1990
and 95% in 1995. They do so to avoid over-representation of each
category. They use first difference log-priced series. With this global
factor, they can explain over 20% of global risky asset price volatility
in their time span. Given the small time frame and VAR analysis
limitations, they estimate a global factor with commodities from the
U.S., Europe, and Japan, spanning back to 1975. This factor covers 60%
of the volatility in this period. The appendix of the paper provides
detailed information on this VAR estimation. To provide more intuition
on this factor, the authors correlate it with some indexes of implied
volatility such as the VIX, outlining its co-movement with common
measures of market variation (in this case a negative correlation). The
global factor will be used later in the impulse-response section.

```{r}
#| echo: false
#| message: false
#| warning: false
#set working directory 
#setwd("/Users/filo/Dropbox/Macrometrics/US-Monetary-Policy-and-the-Global-Financial-Cycle-Replication")
##########################################################################################
  library(readxl)
  library(ggplot2)
  library(rmarkdown)
  library(matrixcalc)
  library(mvtnorm)
  library(parallel)
  library(fredr)
  library(sovereign)
  library(MASS)
  library(patchwork)
  library(fredr)
  library(mgcv)
  library(HDInterval)
##########################################################################################

  url <- "http://silviamirandaagrippino.com/s/DFM-Blocks.zip"
  download.file(url, "MAR(2020)")
  unzip("MAR(2020)")
  data <- read_excel("GFC_VARdata+WEB.xlsx", skip=1, .name_repair = "unique_quiet")
  inst <- read_excel("GFC_VARdata+WEB.xlsx", sheet = 2, skip=1, .name_repair = "unique_quiet")
  #date = ts(seq(from = as.Date("1980-01-01"), to = as.Date("2012-12-01"), by = 'month'))
  #instrument$LABEL <- date #ts object is not able to translate dates
  gf <- read_excel("GFC_VARdata+WEB.xlsx", sheet= 4, skip=2, .name_repair = "unique_quiet", na = "blank")
  #monthly data for monthly FedFundsRate
  # fed <- (fredr(series_id = "FEDFUNDS", 
  #                          observation_start = as.Date("1975-01-01"), 
  #                          frequency = "m",
  #                          aggregation_method = "average"))
  ############################################################
  ############################################################
  #time series of variables 
  #fed       = ts((fed[,3]), start=c(1980,1), frequency=12) #FED federal funds rate
  instrument = ts((inst$FF4), start=c(1980,1), frequency=12) #FFF4 instrument
  indpro     = ts((data$INDPRO), start=c(1980,1), frequency=12) #industrial production
  greaexus   = ts((data$GREAEXUS), start=c(1980,1), frequency=12) #Global Real Economic Activity Ex US
  glbinflows = ts((data$GLBINFLALL), start=c(1980,1), frequency=12) #Global Inflows All Sectors
  dgs1       = ts((data$DGS1), start=c(1980,1), frequency=12) #1 Year Treasury Rate
  pce        = ts((data$PCEPI), start=c(1980,1), frequency=12) #PCE Deflator; 
  bis        = ts((data$BISREER), start=c(1980,1), frequency=12) #BIS real EER
  globalf    = ts((data$GLOBALF), start=c(1980,1), frequency=12) #Global Factor
  globalra   = ts((data$GLOBALRA), start=c(1980,1), frequency=12) #Global Risk Aversion
  glbcrexus  = ts((data$GLBCREXUS), start=c(1980,1), frequency=12) #Global Domestic Credit ex US
  usbdlev    = ts((data$USBDLEV), start=c(1980,1), frequency=12) #Leverage US Brokers and Dealers
  eubdlev    = ts((data$EUBDLEV), start=c(1980,1), frequency=12) #Leverage EU Global Banks
  usbanksl   = ts((data$USBANKSL), start=c(1980,1), frequency=12) #Leverage US Banks
  eubanksl   = ts((data$EUBANKSL), start=c(1980,1), frequency=12) #Leverage EU Banks
  
  #create the bigy matrix with our data
  y = cbind(instrument,
            dgs1, 
            (pce),
            (indpro),
            (greaexus), 
            (glbinflows),
            (bis), 
            globalf, 
            globalra,
            (glbcrexus), 
            (usbdlev), 
            (eubdlev), 
            (usbanksl), 
            (eubanksl))
  
  y.names= cbind("FF4 Instrument", 
                 "1Y Treasury Rate",
                 "PCE", 
                 "Industrial Production", 
                 "Global Real Production ex US",
                 "Global Inflows All Sectors",
                 "BIS EER",
                 "Global Factor",
                 "Global Risk Aversion",
                 "Global Credit ex US",
                 "Leverage US Brokers & Dealers",
                 "Leverage EU Global Banks",
                 "Leverage US Banks",
                 "Leverage EU Banks")
  
    #data in selected window
    y                   <- window((y), start=c(1990,2), end=c(2010,12))
    
  ############################################################
  ############################################################
```

```{r}
#| echo: true 
#| message: false
#| warning: false
#| code-fold: true
#| code-summary: "Global Factors Plot"
ggplot() + 
  geom_line(data = gf, aes(x = `...1`, y = `GLOBAL FACTOR 1975-2010`, linetype = "GF 1975-2010"), color ="blue", na.rm = TRUE, linewidth=1.2) + 
  geom_line(data = gf, aes(x = `...1`, y = `GLOBAL FACTOR 1990-2012`, linetype = "GF 1990-2012"), color="purple", na.rm = TRUE, linewidth=1.2) +
  geom_line(data = data, aes(x = `LABEL`, y = VIX, linetype = "VIX"), color="black", na.rm = TRUE, linewidth=1.2) +
  scale_linetype_manual(values = c("solid", "solid", "solid"), 
                        labels = c("VIX", "GF 1975-2010", "GF 1990-2012"),
                        guide = guide_legend(override.aes = list(color = c("black", "blue", "purple")))) +
  labs(x = "", y = "", title = "Global Factor for Risky Asset Prices") +
  theme(plot.title = element_text(size = 20)) +
  theme(legend.text = element_text(size = 20), legend.title = element_text(size = 0), 
        legend.position = "bottom", plot.title = element_text(hjust = 0.5), 
        panel.border = element_blank(), panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(), axis.line = element_line(colour = "black"),
        panel.background = element_rect(fill = "white"), plot.background = element_rect(fill = "white"), 
        axis.text = element_text(size = 14))


```

## Proxy-VAR Analysis with Rich-Information Bayesian VAR

In this project, we will avoid the computation of the global factor,
because it is out of scope of the Macroeconomerics subject. Instead, we
will concentrate on the Bayesian VAR analysis of Miranda-Agrippino and
Rey. A main reason why the authors studied the monetary effects of U.S.
interest rate changes is that the dollar is the currency of global
banking. A change in FED monetary policy affects banks' borrowing
capacity, the pricing of dollar denominated assets, and cross-border
capital flows. In order to isolate its effects, the two scholars
identify U.S. monetary policy shocks by exploiting 30-min price
revisions around Federal Open Market Committee
([FOMC](https://www.federalreserve.gov/monetarypolicy/fomc.htm))
announcements in the fourth federal funds futures contracts
([FF4](https://www.investopedia.com/terms/f/fed-funds-futures.asp)). The
intuition is that these
[futures](https://www.investopedia.com/terms/f/futures.asp) have an
average maturity of three months, and they can predict revisions of
market expectations about future monetary policy one-quarter in advance.
This assumption holds only if market participants can distinguish
between the systematic component of policy and any observable policy
action. Moreover, with asymmetrical information, the FF4 revisions
contain information about the influence of economic factors relevant to
U.S. monetary policy. Policy announcements provide this information
implicitly.

# The Data

We download the data directly from the website of
[Miranda-Agrippino](http://silviamirandaagrippino.com/code-data). The
two authors study the consequences of a 1% increase in the U.S. monetary
policy considering:

-a domestic VAR with the effects on domestic financial markets and
macroeconomic aggregates in the United States;

-a global VAR with the effects on global asset markets, global domestic
credit and international capital flows;

-a "floaters" VAR to study if a fixed or pegged exchange rate affects
the global contraction.

We will study the global specifications, and include the following
variables:

```{r}
#| echo: true 
#| message: false
#| warning: false
#| code-fold: true
#| code-summary: "Data Plots"
  library(ggplot2)
  library(patchwork)
  ############################################################
  ############################################################  
fed.gdp.plot <-  (ggplot(data=data) + 
    geom_line(aes(x = `LABEL`, y = `DGS1`, linetype = "solid"), na.rm = TRUE, color = "blue") 
    + geom_line(aes(x = `LABEL`, y = `GREAEXUS`, linetype = "dashed"), na.rm = TRUE, color = "black")+
    scale_linetype_manual(values=c("solid", "dashed"), name="", labels=c("Global Real Economic Activity Ex US", "1 Year Treasury Rate"), guide = guide_legend(override.aes = list(color = c("blue", "black")), nrow=2)) +
    guides(linetype = guide_legend(override.aes = list(color = c("black", "blue")), nrow=2)) +
    labs(x ="", y="", title= "Treasury Rate and Global \nReal Activity Index") +
      theme(legend.text = element_text(size=6),legend.title = element_text(size=5, face="bold"),  legend.position = "bottom", plot.title = element_text(hjust = 0.5, size=11), panel.border = element_blank(), panel.grid.major = element_blank(),
            panel.grid.minor = element_blank(), axis.line = element_line(colour = "black"),panel.background = element_rect(fill = "white"),
            plot.background = element_rect(fill = "white"), axis.text =element_text(size = 10)))
  ############################################################
  ############################################################
  pce.plot <- (ggplot(data=data) +
     geom_line(aes(x = `LABEL`, y = PCEPI, linetype = "solid"), na.rm = TRUE, color = "blue") +
     scale_linetype_manual(values=c("solid"), name="", labels=c("PCE Deflator"), guide = guide_legend(override.aes = list(color = c("blue")))) +
     guides(linetype = guide_legend(override.aes = list(color = c("blue")))) +
     labs(x ="", y="", title= "Price Deflator") +
       theme(legend.text = element_text(size=10),legend.title = element_text(size=16, face="bold"),  legend.position = "bottom", plot.title = element_text(hjust = 0.5), panel.border = element_blank(), panel.grid.major = element_blank(),
             panel.grid.minor = element_blank(), axis.line = element_line(colour = "black"),panel.background = element_rect(fill = "white"),
             plot.background = element_rect(fill = "white"), axis.text = element_text(size = 10)))
  
  
  ############################################################
  ############################################################
  leverage.plot <-  (ggplot(data=data) + 
    geom_line(aes(x = `LABEL`, y = EUBDLEV, linetype = "solid"), na.rm = TRUE, color = "blue", linewidth=1.1) + 
    geom_line(aes(x = `LABEL`, y = USBANKSL, linetype = "dashed"), na.rm = TRUE, color = "black", linewidth=1.1) + 
    geom_line(aes(x = `LABEL`, y = EUBANKSL, linetype = "twodash"), na.rm = TRUE, color = "purple", linewidth=1.1) +
    geom_line(aes(x = `LABEL`, y = USBDLEV, linetype = "dotted"), na.rm = TRUE, color = "violet", linewidth=1.1) +
    scale_linetype_manual(values=c("solid", "dashed", "twodash", "dotted"), name="", labels=c("Leverage EU Banks", "Leverage EU Global Banks", "Leverage US Banks", "Leverage US Brokers and Dealers"), guide = guide_legend(override.aes = list(color = c("blue", "black", "purple", "violet")))) +
    guides(linetype = guide_legend(override.aes = list(color = c("black", "blue", "purple", "violet")))) +
    labs(x ="", y="", title= "Leverage") +
      theme(legend.text = element_text(size=7),legend.title = element_text(size=16, face="bold"),  legend.position = "bottom", plot.title = element_text(hjust = 0.5), panel.border = element_blank(), panel.grid.major = element_blank(),
            panel.grid.minor = element_blank(), axis.line = element_line(colour = "black"),panel.background = element_rect(fill = "white"),
            plot.background = element_rect(fill = "white"), axis.text = element_text(size = 10)))
  ############################################################
  ############################################################
  mixed.types.plot <-   (ggplot(data=data) + 
     geom_line(aes(x = `LABEL`, y = GLBINFLALL, linetype = "solid"), na.rm = TRUE, color = "blue", linewidth=1.1) + 
     geom_line(aes(x = `LABEL`, y = INDPRO, linetype = "dashed"), na.rm = TRUE, color = "black", linewidth=1.1) + 
     geom_line(aes(x = `LABEL`, y = GLBCREDIT, linetype = "twodash"), na.rm = TRUE, color = "purple", linewidth=1.1) +
       geom_line(aes(x = `LABEL`, y = BISREER, linetype = "dotted"), na.rm = TRUE, color = "violet", linewidth=1.1) +
     scale_linetype_manual(values=c("solid", "dashed", "twodash", "dotted"), name="", labels=c("Industrial Production","Global Inflows All Sectors", "Global Domestic Credit", "BIS real EER"), guide = guide_legend(override.aes = list(color = c("blue", "black", "purple", "violet")))) +
     guides(linetype = guide_legend(override.aes = list(color = c("black", "blue", "purple", "violet")))) +
     labs(x ="", y="", title= "Production, Inflows, Credit, BIS REER") +
       theme(legend.text = element_text(size=8),legend.title = element_text(size=16, face="bold"),  legend.position = "bottom", plot.title = element_text(hjust = 0.5), panel.border = element_blank(), panel.grid.major = element_blank(),
             panel.grid.minor = element_blank(), axis.line = element_line(colour = "black"),panel.background = element_rect(fill = "white"),
             plot.background = element_rect(fill = "white"), axis.text = element_text(size = 10)))
  
  ############################################################
  ############################################################
  risk.types.plot <-  (ggplot(data=data) + 
    geom_line(aes(x = `LABEL`, y = GLOBALF, linetype = "solid"), na.rm = TRUE, color = "blue") + 
    geom_line(aes(x = `LABEL`, y = GLOBALRA, linetype = "dashed"), na.rm = TRUE, color = "black") +
    scale_linetype_manual(values=c("solid", "dashed"), name="", labels=c("Global Risk Aversion", "Global Factor"), guide = guide_legend(override.aes = list(color = c("black", "blue")), nrow=2)) +
    labs(x ="", y="", title= "Risk Aversion") +
      theme(legend.text = element_text(size=9),legend.title = element_text(size=16, face="bold"),  legend.position = "bottom", plot.title = element_text(hjust = 0.5), panel.border = element_blank(), panel.grid.major = element_blank(),
            panel.grid.minor = element_blank(), axis.line = element_line(colour = "black"),panel.background = element_rect(fill = "white"),
            plot.background = element_rect(fill = "white"), axis.text = element_text(size = 10)))
  ############################################################
  ############################################################
  ############################################################
  ############################################################
  
  #grid.arrange(global.credit, global.inflows, leverage.types, production.types, risk.types, nrow = 5)
  library(patchwork)
  combinedplot.1 <- mixed.types.plot / leverage.plot 
    
  combinedplot.2 <- fed.gdp.plot + risk.types.plot + pce.plot
  
  combinedplot.1
  combinedplot.2
```

## Data Description

Against this framework, our interest is to study the degree by which
U.S. monetary policy shocks can propagate throughout the world's
economies. In order to have a global picture we study macroeconomic
variables that can affect and influence a FED change in the interest
rate. The global variables are constructed by scrutinizing these
countries: Argentina, Australia, Austria, Belarus, Belgium, Bolivia,
Brazil, Bulgaria, Canada, Chile, Colombia, Costa Rica, Croatia, Cyprus,
Czech Republic, Denmark, Ecuador, Finland, France, Germany, Greece, Hong
Kong, Hungary, Iceland, Indonesia, Ireland, Italy, Japan, Latvia,
Lithuania, Luxembourg, Malaysia, Malta, Mexico, Netherlands, New
Zealand, Norway, Poland, Portugal, Romania, Russia, Serbia, Singapore,
Slovakia, Slovenia, South Africa, South Korea, Spain, Sweden,
Switzerland, Thailand, Turkey, U.K., and the U.S.

As discussed in the previous section, we are ìncluding the variables of
the global VAR of @miranda2020us. In the first graph, we include the
following variables:

-the U.S. industrial production index that measures the real output of
all relevant establishments located in the U.S.; -the Bank for
International Settlement (BIS) effective exchange rate (EER) for the
United States, i.e. a summary measure calculated by the BIS to account
for changes in the U.S. bilateral exchange rate against other countries
by their trade importance; -global inflows, defined as direct
cross-border credit flows from the U.S. to the aforementioned countries'
banks and non-bank recipients. This variable is key to explaining the
degree of financial dependency of the rest of the world vis-à-vis the
financial hegemon; -global domestic credit, another key variable in our
VAR that outlines the total amount of funds in the world economy,
including loans, debt instruments, and other forms of credit provided by
financial institutions.

In the second graph, we outline four types of leverage. First of all,
leverage can be defined as the ratio between assets and equity, with
equity being the difference between assets and debts. In finance, this
measurement refers to the use of borrowed funds to finance assets or
investments. Against this backdrop, the authors construct this variable
as the ratio between claims in the private sector, i.e. credit extended
by banks and other financial institutions to the private sector, and the
sum of transferable deposits held by depository corporations, excluding
central banks. This ratio reflects the proportion of credit extended to
the private sector relative to deposits held by depository corporations,
excluding central banks. A higher ratio suggests a higher banking
leverage level. The authors differentiate between different leverages
given financial agents' risk-taking behavior. As a matter of fact, we
can observe that the leverage between EU global banks and US brokers and
dealers is much larger than the leverage of big but not systemic EU and
US banks. EU global banks include systematically important banks such as
UBS and Unicredit. Global banks' leverage is high for several reasons
such as size and risk appetite. Brokers and dealers' behavior can be
explained by their risk-loving approach.

In the third graph, we include the FED policy rate coupled with the
global real economic activity index (excluding the US). We can observe
the lag effect of an increase in interest rates with a decrease in
international economic activity. In the fourth graph we include the
global factor discussed in the previous section, plotted with global
risk aversion (one variable is the inverse of the other). The factor is
rotated to provide an intuitive view that an increase in the factor
(risk aversion) reflects an increase in world asset prices. Lastly, we
plot the U.S. PCE deflator to measure changes in goods and services
prices over time.

Lastly, we will add the aforementioned instrument on top of the $y_t$
variable. In the next section we will provide the reason for this
procedure.

## Order of Integration of the Variables

We run an augmented Dickey Fuller (ADF) test to determine the order of
integration of our variables. The test rejects the null hypothesis that
the variables are unit-root except for our instrument variable as
expected. In order to provide more insight into the order of integration
we re-run an ADF test with the first difference of the variables.

```{r}
#| echo: true 
#| message: false
#| warning: false
#| code-fold: true
#| code-summary: "ADF-tests"
  library(tseries)
  adf <- as.data.frame(matrix(nrow=14,ncol=3,NA))
  rownames(adf) <- colnames(y[1:14])
  colnames(adf) <- c("Dickey-Fuller","Lag-order", "p-value")
  
  for (i in 1:ncol(y)){
    adf_tmp                 <-  adf.test(y[,i])
    adf[i,"Dickey-Fuller"]  <-  round(as.numeric(adf_tmp[1]),3)
    adf[i,"Lag-order"]      <-  as.numeric(adf_tmp[2])
    adf[i,"p-value"]        <-  round(as.numeric(adf_tmp[4]),3)
  }
  
  adf.diff <- as.data.frame(matrix(nrow=14,ncol=3,NA))
  rownames(adf.diff) <- colnames(y[1:14])
  colnames(adf.diff) <- c("Dickey-Fuller diff","Lag-order diff", "p-value diff")
  
  for (i in 1: ncol(y)){
    tmp.diff                         <-  adf.test(diff(y[,i]))
    adf.diff[i,"Dickey-Fuller diff"] <-  round(as.numeric(tmp.diff[1]),3)
    adf.diff[i,"Lag-order diff"]     <-  as.numeric(tmp.diff[2])
    adf.diff[i,"p-value diff"]       <-  round(as.numeric(tmp.diff[4]),3)
  }
  tab = cbind(t(y.names),adf, adf.diff)
  colnames(tab)[1] <- "Variable Names"
  knitr::kable(tab, index=TRUE)

```

# The Model

## BVAR Framework

From @HRW2022ProxySVARs we define the model as: $$
y_t=B_0B_1y_{t-1}+...+B_0B_py_{t-p}+B_0\varepsilon_t, t=1,..,T $$
$$y_t=A_1y_{t-1}+...+A_py_{t-p}+u_t$$ where
$A_j, : \left \{j=1,2,\dots,p\right \}$ where $A(L)=B_0B(L)$ are a K x K
coefficient matrices, and L is the lag operator, and $u_t$ in the first
reduced form is serially uncorrelated with zero mean and positive
definite (non-diagonal) covariance matrix $\Sigma_u.$ The structural
shocks $\varepsilon_t$ in the second reduced form are assumed to be
mutually uncorrelated and normalised to have unit variance.
$\Xi\left(\varepsilon_t \varepsilon_t' \right)=I_K$. Structural shocks
are mapped to the reduced-form system through a K x K non-singular
matrix $B_0$, such that $B_0^{-1}B_0^{-1'}=\Sigma_u$. For simplicity,
the process is assumed to be causal and
$detA(z)=det(I_k-\sum_{j=1}^{p}A_jz^{j})\neq 0$ for
$\left| z \right|\le 1$. This ensures that the process has a Wold moving
average MA representation. Moreover,

```{=tex}
\begin{align*}
y_t=\mu+\sum_{i=0}^{\infty}\Phi_iu_{t-i}=\mu+\sum_{i=0}^{\infty}\Phi_iB_0\varepsilon_{t-i}=
\mu+\sum_{i=0}^{\infty}\Theta_i\varepsilon_{t-i}.
\end{align*}
```
With: \begin{gather}
\mu=A(1)^{-1}\nu, ::: \Phi_0=I_K, \ \Phi_i=\sum_{j=1}^{i}A_j\Phi_{i-j}, ::: A_j=0 :for: j>p.
\end{gather} The second to last MA representation is of particular
importance because the structural MA coefficients $\Theta_i=\Phi_iB_0$
cannot be recovered without a proper identification. We will briefly
outline the Proxy SVAR approach.

Let $z_t$ be an external instrument to identify the structural shock of
interest $\varepsilon_{kt}, k\:\epsilon \: \left \{1,\dots, K\right \}$.
$z_t$ has to satisfy the *relevant* condition
$\Xi(\varepsilon_{kt}z_t)=\phi \neq 0$ and the *exogeneity* condition
$\Xi(\varepsilon{lt}z_t)=0, \forall l\:\epsilon\left \{1,\dots, K\right \}\setminus\left \{k\right \}$.

From these conditions, it follows that the population covariance between
the instrument and VAR residuals obtain the k-th column if $B_0$,
denoted by $B_{0k}$. \\end{gather} The second to last MA representation
is of particular importance because the structural MA coefficients
$\Theta_i=\Phi_iB_0$ cannot be recovered without a proper
identification. We will briefly outline the Proxy SVAR approach. Let
$z_t$ be an external instrument to identify the structural shock of
interest $\varepsilon_{kt}, k\:\epsilon \: \left \{1,\dots, K\right \}$.
$z_t$ has to satisfy the *relevant* condition
$\Xi(\varepsilon_{kt}z_t)=\phi \neq 0$ and the *exogeneity* condition
$\Xi(\varepsilon{lt}z_t)=0, \forall l\:\epsilon\left \{1,\dots, K\right \}\setminus\left \{k\right \}$.
From these conditions, it follows that the population covariance between
the instrument and VAR residuals obtains the k-th column of $B_0$,
denoted by $B_{0k}$.

$$
\Xi(u_tz_t) = B_{0,k} \\
\Xi(\varepsilon{kt}z_t)=\phi B_{0,k}
$$ Moreover, let $\Pi$ denotes the $1$x$K$ coefficient vector from the
regression of the instrument on the residual vector $u_t$ gives the
shock $\varepsilon_{kt}$ up to a scale $\phi$.
$\Pi u_t=\Xi(z_tu^{'}_t)\Sigma_{u}^{-1}u_t=\phi B^{'}_{0,k}\left [B_0B_0^{'} \right ]u_t=\phi e^{'}_t\varepsilon_{kt}$
@MollerWolf2021LPVARS, exploiting their result that Local Projections
and VAR impulse response function are equal up to a constant of
proportionality, show that proxy SVARS impulse responses can be computed
putting the instrument in the first row of the data vector $y_t$ in a
SVAR framework. This result follows from the invertibility of
$\varepsilon$ and two assumptions: -the data $y_t$ is
covariance-stationary; -the data $y_t$ is a jointly Gaussian vector time
series.

In our Bayesian approach these requirements are met when we define the
distributions of our error terms. We will outline them in the next
section.

## Basic Model

We specify our model to follow a matrix-variate normal distribution
\begin{gather}
Y = XA + E \\
\\ E|X \sim MN_{T \times N}(0_{T \times N},\Sigma,I_T) 
\end{gather} Given that the function Y is a linear combination of the
error terms E, we can specify \begin{gather}
Y|X,A,\sim MN_{T \times N}(XA,\Sigma,I_T)
\end{gather} Hence, the Likelihood function follows a
Matrix-Variate-Normal form: \begin{gather}
L(A,\Sigma|Y,X) \propto det(\Sigma)^{-\frac{T}{2}} exp \left\{-\frac{1}{2} tr \left[ \Sigma^{-1}(Y-XA)'(Y-XA) \right] \right\} \\
\\ \propto det(\Sigma)^{-\frac{T}{2}} exp \left\{-\frac{1}{2} tr \left[ \Sigma^{-1}(A-\hat{A})'X'X(A-\hat{A}) \right] \right\} exp \left\{-\frac{1}{2} tr \left[\Sigma^{-1}(Y-X \hat{A})'(Y-X \hat{A}) \right] \right\} \\
\end{gather} where from Maximum Likelihood Estimation we have obtained
\begin{gather}
\hat{A} = (X'X)^{-1}X'Y \\
\\ \hat{\Sigma} = \frac{1}{T} (Y-X \hat{A})'(Y-X \hat{A})
\end{gather} In the basic model, our prior follows a natural-conjugate
prior distribution of the same form: \begin{gather}
p(A,\Sigma) = p(A|\Sigma) p(\Sigma) \\
\\ A|\Sigma \sim MN_{K \times N} (\underline{A}, \Sigma , \underline{V}) \\
\\ \Sigma \sim IW_{N}(\underline{S},\underline{\nu})
\end{gather}

With parameters: \begin{gather}
\underline{A} = [0_{N \times 1} \quad I_N \quad 0_{N \times (p-1)N}]' \\ 
\\ Var[vec(A)] = \Sigma \otimes  \underline{V} \\
\\ \underline{V} = diag([\kappa_2 \quad \kappa_1 (p^{-2} \otimes I_N)]) \\
\\ p = [1,2,...p]
\end{gather} $\kappa_2$ and $\kappa_1$ describe, respectively, the
priors of the overall shrinkage level of the constant term and the
variance-covariance matrix of the autoregressive slopes for the constant
term. We specify $\kappa_2=1$ and $\kappa_1=0.02$ to respect
@MollerWolf2021LPVARS assumptions. We would not expect persistence in
each lag for stationary variables following a random walk. Moreover, we
set the prior of the autoregressive parameters $A$ equal to a vector of
zeros. The resulting full conditional posterior is: \begin{gather}
p(A,\Sigma|Y,X) = p(A|Y,X,\Sigma)p(\Sigma|Y,X) \\
\\ p(A|Y,X,\Sigma) = MN_{K \times N}(\bar{A}, \Sigma, \bar{V}) \\ 
\\ p(\Sigma | Y, X) = IW_N(\bar{S},\bar{\nu})
\end{gather}

We can derive the full conditional posterior: \begin{gather}
P(A,\Sigma|Y,X) \propto L(A,\Sigma|Y,X)p(A,\Sigma) \\
\\ \propto L(A,\Sigma|Y,X)p(A|\Sigma)p(\Sigma) \\
\\ det(\Sigma)^{-\frac{T}{2}} \times exp \left\{-\frac{1}{2} tr \left[ \Sigma^{-1}(A-\hat{A})' X'X (A-\hat{A})\right] \right\} \\
\\ \times exp\left\{-\frac{1}{2}tr \left[ \Sigma^{-1}(Y-X\hat{A})'(Y-X\hat{A}) \right] \right\} \\
\\ \times det(\Sigma)^{-\frac{N+K+\underline{\nu}+1}{2}} \\
\\ \times exp\left\{-\frac{1}{2}tr \left[ \Sigma^{-1}(A-\underline{A})'\underline{V}^{-1}(A-\underline{A}) \right] \right\} \\ 
\\ \times exp \left\{ -\frac{1}{2} tr \left[ \Sigma^{-1} \underline{S} \right] \right\}
\end{gather}

After some calculations:

```{=tex}
\begin{gather}\\
p(A,\Sigma|Y,X) \propto \\
\\ det{(\Sigma)}^{-\frac{T+N+K+ \underline{\nu}
+1}{2}} \times exp\left\{ -\frac{1}{2} tr \left[ \Sigma^{-1} \left[ (A-\bar{A})^{'} \bar{V}^{-1} (A-\bar{A})+\underline{S} +Y^{'}Y + \underline{A}^{'} \underline{V}^{-1}\underline{A} -\bar{A}^{'} \bar{V}^{-1}\bar{A}\right]\right]\right\}
\end{gather}
```
where the full conditional posterior has the same natural-conjugate form
of our prior:

```{=tex}
\begin{gather}
p(A,\Sigma|Y,X) = p(A|Y,X,\Sigma)p(\Sigma|Y,X) \\
\\ p(A|Y,X,\Sigma) = MN_{K \times N}(\bar{A}, \Sigma, \bar{V}) \\
\\ p(\Sigma | Y, X) = IW_N(\bar{S},\bar{\nu})\end{gather}
```
with posterior parameters:

```{=tex}
\begin{gather}
\bar{V} = (X^{'}X+ \underline{V}^{-1})^{-1} \\ 
\\ \bar{A} = \bar{V}(X^{'}Y+\underline{V}^{-1} \underline{A}) \\ 
\\ \bar{\nu} = T + \underline{\nu} \\ 
\\ \bar{S} = \underline{S} + Y^{'}Y +  \underline{A}^{'}\underline{V}^{-1}\underline{A} - \bar{A}^{'}\bar{V}^{-1}\bar{A}
\end{gather}
```
In order to compute our posterior parameters in R, we first specify
values for our priors, then calculate the posteriors and draw $A$ and
$\Sigma$ respectively from Matrix-Variate-normal and Inverse Wishart
distributions. At this point, we can obtain our structural parameters
through a Cholesky decomposition of our matrix $\Sigma$, namely with
$\Sigma^{-1}=(B_0B_0^{'})$.

```{r}
#| echo: true 
#| message: false
#| warning: false
#| code-fold: true
#| code-summary: "Basic Model Function"
  # setup function for our analysis
  basic.model <- function(bigy, p, S, start, end){
  ############################################################
  N       = ncol(bigy)
  K       = 1+N*p
  ############################################################
  Y       = bigy[(p+1):nrow(bigy),]
  X       = matrix(1,nrow(Y),1)
  for (i in 1:p){
    X     = cbind(X,bigy[(p+1):nrow(bigy)-i,])
  }
  
  A.prior     = matrix(0,K,N)
  A.prior[2:(N+1),] <- diag(c(0,rep(1,N-1))) #0 for the instrument (stationary variable)
  #A.prior[2:(N+1),] <- diag(c(0,rep(1,N-1))) #0 for the instrument (stationary variable)
  V.prior     = (diag(c(1,0.02*((1:p)^(-2))%x%rep(1,N)))) #1 is kappa.2, 0.02 is kappa.1
  S.prior     = diag(N)
  nu.prior    = N+1
  
  # normal-inverse Wishart posterior parameters
  ############################################################
  V.bar.inv   = t(X)%*%X + diag(1/diag(V.prior)) #X'X+diag(V^-1)
  V.bar       = solve(V.bar.inv) #inv(X'X+diag(V^-1))
  A.bar       = V.bar%*%(t(X)%*%Y + diag(1/diag(V.prior))%*%A.prior) #V.bar(X'X+diag((V.prior)^-1(A.prior)))
  nu.bar      = nrow(Y) + nu.prior 
  S.bar       = S.prior + t(Y)%*%Y + t(A.prior)%*%diag(1/diag(V.prior))%*%A.prior - t(A.bar)%*%V.bar.inv%*%A.bar
  #S.prior+Y'Y+(A.prior)'*diag(diag((V.prior)^-1))*A.prior-A.bar'*(X'X+diag(V^-1))*A.bar
  S.bar.inv   = solve(S.bar)
  
  # posterior draws
  ############################################################
  Sigma.posterior   = rWishart(S, df=nu.bar, Sigma=S.bar.inv)
  Sigma.posterior   = apply(Sigma.posterior,3,solve)
  Sigma.posterior   = array(Sigma.posterior,c(N,N,S))
  A.posterior       = array(rnorm(prod(c(dim(A.bar),S))),c(dim(A.bar),S)) #3 dimensional, S repetitions of rnorm
  B.posterior       = array(NA,c(N,N,S))
  L                 = t(chol(V.bar))
  for (s in 1:S){
    cholSigma.s     = chol(Sigma.posterior[,,s])
    B.posterior[,,s]= t(cholSigma.s)
    A.posterior[,,s]= A.bar + L%*%A.posterior[,,s]%*%cholSigma.s
}
  #return the parameters of interest
  results = list("B" = B.posterior, "A" = A.posterior )
  return(results)
  }
```

## Extended Model

In the extended model, we set hyperprior parameters for the
autoregressive parameter $\kappa_A$ to follow an Inverse Gamma 2
distribution $IG2(\underline{S}_{\kappa},\underline{\nu}_{\kappa})$ and
$\kappa_{\Sigma}$ hyperprior parameter for the variance-covariance
matrix to follow a Gamma Distribution
$G(\underline{S}_{\Sigma},\underline{a}_{\Sigma})$. We define a
hierarchical model to provide flexibility to our model and hopefully
reduce uncertainty. We will define the full conditional posterior of the
hyperparameter $\kappa_A$ first.

```{=tex}
\begin{gather}
p(\kappa_A | A,\Sigma, Y,X) \propto L(Y|X,A,\Sigma) \times p(\kappa_A) \times p(A|\Sigma,\kappa_A) \times p(\Sigma|\kappa_A) \\ 
\\ \propto p(\kappa_A) \times p(A|\Sigma,\kappa_A) \\ 
\\ \propto (\kappa_A)^{-\frac{\underline{\nu_a}+2}{2}} 
exp\left\{ -\frac{1}{2} \frac{\underline{S_a}}{\kappa_A} \right\}  \times exp\left\{-\frac{1}{2}tr\left[\Sigma^{-1}(A-\underline{A_{\kappa}})^{'} \frac{1}{\kappa}(\underline{V_{\kappa}})^{-1} (A-\underline{A})\right]\right\} \times det(\kappa_A \underline{V})^{-\frac{N}{2}} \\ 
\\ \propto (\kappa_A)^{-\frac{\underline{\nu_a}_{\kappa_A}+2+NK}{2}} exp \left\{-\frac{1}{2} \frac{1}{\kappa_A} \left[\underline{S_a}_{\kappa_A} + tr \left[\Sigma^{-1}(A-\underline{A})^{'}\underline{V}^{-1}(A-\underline{A}) \right]\right] \right\}
\end{gather}
```
where we recognise the kernel of an Inverse Gamma 2 Distribution with

```{=tex}
\begin{gather}
\bar{S}_a = \underline{S}_a+tr \left[ \Sigma^{-1} (A-\underline{A})^{'} \underline{V}^{-1} (A-\underline{A})\right] \\
\\ \bar{\nu}_a = \underline{\nu}_a+NK
\end{gather}
```
In addition, we obtain a similar full conditional posterior of the
hyperparameter $\kappa_{\Sigma}$. \begin{gather}
p(\kappa_{\Sigma} | A,\Sigma, Y,X) \propto L(Y|X,A,\Sigma) \times p(\kappa_{\Sigma}) \times p(A|\Sigma,\kappa_A) \times p(\Sigma|\kappa_{\Sigma}) \times p(\kappa_A) \\
\\ \propto p(\kappa_{\Sigma}) \times p(\Sigma|\kappa_{\Sigma}) \\ 
\\ \propto (\kappa_{\Sigma})^{-\frac{\underline{\nu}N}{2}} 
exp\left[{ -\frac{1}{2} \frac{\kappa_{\Sigma}}{tr\left [\Sigma^{-1}\underline{s}_{\Sigma} \right ]^{-1}}} \right\}  \times (\kappa_{\Sigma})^{\underline{a}_{\Sigma}-1}exp\left [{-\frac{\kappa_{\Sigma}}{\underline{s}_{\Sigma}}}  \right ] \\
\\ \propto (\kappa_{\Sigma})^\frac{{}{N\underline{\nu}+2\underline{a}_{\Sigma}-2}}{2}exp\left [{-\frac{\kappa_{\Sigma}}{\left [2tr\left[\Sigma^{-1}\underline{s}_{\Sigma} \right ]^{-1}+\left[\underline{s}_{\Sigma} \right ]^{-1}\right ]^{-1}}}  \right ] \\
\end{gather} where we can recognise the kernel of an Gamma Distribution
with \begin{gather}
\bar{S}_{\Sigma} = {\left [2\left[tr\Sigma^{-1}\underline{s}_{\Sigma} \right ]^{-1}+\left[\underline{s}_{\Sigma} \right ]^{-1}\right ]^{-1}} \\
\\ \bar{a}_{\Sigma} = \frac{N\underline{\nu}}{2}+\underline{a}_{\Sigma}
\end{gather}

Therefore, our new full conditional posterior distribution will be as
following: The full conditional posterior of $(A,\Sigma)$ is:
\begin{gather}
p(A,\Sigma|X,Y,\kappa_a,\kappa_{\Sigma}) \propto L(A,\Sigma|Y,X) \times p(A|\Sigma,\kappa_a) \times p(\Sigma|\kappa_{\Sigma}) \\
\\ \propto det(\Sigma)^{-\frac{K}{2}} exp \left\{-\frac{1}{2}tr \left[ \Sigma^{-1}(Y-XA)^{'}(Y-XA)\right] \right\} \\
\\ \times exp \left\{ -\frac{1}{2} tr \left[ \Sigma^{-1}(A-\underline{A})^{'}(\kappa_a \underline{V})^{-1}(A-\underline{A}) \right] \right\} \\
\\ \times det(\Sigma)^{\frac{\underline{\nu}+N+1}{2}} exp\left\{-\frac{1}{2} tr \left[\Sigma^{-1}\kappa_{\Sigma} \right] \right\}
\end{gather}

We recognize kernel of matrix-normal inverse Wishart distribution, with
parameters as follows: \begin{gather}
\bar{V} = (X'X+(\kappa_a \underline{V}))^{-1} \\ 
\\ \bar{A} = \bar{V}(X'Y+(\kappa_a \underline{V}^{-1}\underline{A})) \\ \\ \bar{S} = I_N\kappa_{\Sigma}+Y'Y+\underline{A}^{'}(\kappa_a \underline{V})^{-1}\underline{A} - \bar{A}^{'} \bar{V}^{-1}\bar{A} \\ 
\end{gather} where we can recognise the kernel of an Gamma Distribution
with \begin{gather}
\bar{s}_{\Sigma} = {\left [2\left[tr\Sigma^{-1}\underline{s}_{\Sigma} \right ]^{-1}+\left[\underline{s}_{\Sigma} \right ]^{-1}\right ]^{-1}} \\
\\ \bar{a}_{\Sigma} = \frac{N\underline{\nu}}{2}+\underline{a}_{\Sigma}
\end{gather}

Therefore, our new full conditional posterior distribution will be as
follows: \begin{gather}
p(A,\Sigma|X,Y,\kappa_A,\kappa_{\Sigma}) \propto L(A,\Sigma|Y,X) \times p(A|\Sigma,\kappa_A) \times p(\Sigma|\kappa_{\Sigma}) \\
\\ \propto det(\Sigma)^{-\frac{K}{2}} exp \left\{-\frac{1}{2}tr \left[ \Sigma^{-1}(Y-XA)^{'}(Y-XA)\right] \right\} \\
\\ \times exp \left\{ -\frac{1}{2} tr \left[ \Sigma^{-1}(A-\underline{A})^{'}(\kappa_A \underline{V})^{-1}(A-\underline{A}) \right] \right\} \\
\\ \times det(\Sigma)^{\frac{\underline{\nu}+N+1}{2}} exp\left\{-\frac{1}{2} tr \left[\Sigma^{-1}\kappa_{\Sigma} \right] \right\}
\end{gather}

We recognize the kernel of a matrix-normal inverse Wishart distribution,
with parameters as follows: \begin{gather}
\bar{V} = (X'X+(\kappa_A \underline{V}))^{-1} \\ 
\\ \bar{A} = \bar{V}(X'Y+(\kappa_A \underline{V}^{-1}\underline{A})) \\ \\ \bar{S} = I_N\kappa_{\Sigma}+Y'Y+\underline{A}^{'}(\kappa_A \underline{V})^{-1}\underline{A} - \bar{A}^{'} \bar{V}^{-1}\bar{A} \\ 
\\ \bar{\nu} = T + \underline{\nu}
\end{gather}

## Gibbs Sampler

In order to reach our analytical solutions we use a Monte Carlo Markov Chain (MCMC) method, namely the Gibbs Sampler procedure. We generate random draws
from the joint posterior distribution and update them at each iteration
to compute our posterior distribution parameters. In our case we exploit
the following procedure: Initialize $\kappa_A$ and $\kappa_{\Sigma}$ at
$\kappa_A^{(0)}$ and $\kappa^{(0)}_{\Sigma}$.

At each iteration s:

1.  Draw
    $(A,\Sigma)^{(s)} \sim p(A,\Sigma\| X,Y,\kappa*a^{(s-1)},* \kappa{\Sigma}^{(s-1)})$
2.  Draw $\kappa_A^{(s)} \sim p(\kappa_A|Y,X,A,\Sigma)$
3.  Draw $\kappa_{\Sigma}^{(s)} \sim p(\kappa_{\Sigma}|Y,X,A,\Sigma)$

Repeat steps 1 and 2 for $(S_1 + S_2)$ times. Discard the first $S_1$
repetitions. Return the output as
$\left \{ A^{(s)}, \Sigma^{(s)} \right \}^{S2}_{s=S1+1}$.

```{r}
#| echo: true 
#| message: false
#| warning: false
#| code-fold: true  
#| code-summary: "Extended Model Function 2"
extended.model <- function(bigy, p, S1, S2){
  ############################################################
  Y       = bigy[(p+1):nrow(bigy),]
  X       = matrix(1,nrow(Y),1)
  for (i in 1:p){
    X     = cbind(X,bigy[(p+1):nrow(bigy)-i,])
  }
  
  N       = ncol(bigy)
  K       = 1+N*p
  S=S1+S2 #sum of replications
  # set the priors
  A.prior     = matrix(0,K,N) #try to put is as 0
  #A.prior[2:(N+1),] <- diag(c(0,rep(1,N-1)))
  V.prior         = (diag(c(1,0.02*((1:p)^(-2))%x%rep(1,N)))) #1 is kappa.2, 0.02 is kappa.1
  nu.prior        = N+1
  #kappa.a priors
  nu.prior.a      = 3    #ig2
  s.a.prior       = 0.5
  #kappa.sigma priors
  a.sigma.prior   = 1  #rgamma
  s.sigma.prior   = 0.01
  
  A.posterior         = array(rnorm(prod(c(K,N,S))), c(K, N, S))
  B.posterior         = array(NA,c(N,N,S))
  Sigma.post          = array(NA,c(N,N,S))
  
  k.sigma             = matrix(NA, S, 1)
  k.a                 = matrix(NA, S, 1)
  k.sigma[1] <- 1
  k.a[1]     <- 1
  
  nu.bar      = nrow(Y) + nu.prior
  nu.bar.a    = nu.prior.a + N*K
  a.sigma.bar = (N*nu.prior)/2+a.sigma.prior
  
  for (s in 1:S) {
    # normal-inverse Wishart, IG2, and Gamma posterior parameters
    ############################################################
    V.bar.inv   = t(X)%*%X + (k.a[s]^-1)*diag(1/diag(V.prior))
    V.bar       = solve(V.bar.inv)
    A.bar       = V.bar%*%(t(X)%*%Y + diag(1/diag(k.a[s]*V.prior))%*%A.prior)
    S.bar       = k.sigma[s]*diag(N) + t(Y)%*%Y + t(A.prior)%*%diag(1/diag(k.a[s]*V.prior))%*%A.prior - t(A.bar)%*%V.bar.inv%*%A.bar
    S.bar.inv   = solve(S.bar)
    # posterior draws
    ############################################################
    Sigma.posterior.inv = rWishart(1, df=nu.bar, Sigma=S.bar.inv)
    Sigma.posterior     = solve(Sigma.posterior.inv[,,1])
    # Sigma.posterior     = matrix(Sigma.posterior, N, N)
    L                   = t(chol(V.bar))
    cholSigma.s         = chol(Sigma.posterior)
    A.posterior[,,s]    = A.bar + L%*%A.posterior[,,s]%*%cholSigma.s
    s.a.bar       <- s.a.prior + sum(diag(Sigma.posterior.inv[,,1] %*% t(A.posterior[,,s]-A.prior)%*% diag(1/diag(V.prior)) %*% (A.posterior[,,s]-A.prior)))
    s.sigma.bar   <- (2*(sum(diag(Sigma.posterior.inv[,,1]))^-1)+((s.sigma.prior)^-1))^-1
    if (s <= S){
      #draw k.a from IG2
      k.a[s+1]      <- s.a.bar / rchisq(1, df=nu.bar.a)
      #draw k.sigma from gamma distribution
      k.sigma[s+1]  <- rgamma(1, shape = a.sigma.bar, scale = s.sigma.bar)
    }
    
    B.posterior[,,s]  = t(chol(Sigma.posterior))
    Sigma.post[,,s]   = Sigma.posterior
  }
  #return the parameters of interest
  results = list("B" = B.posterior[,,S1+1:S2], 
                 "A" = A.posterior[,,S1+1:S2], 
                 "Sigma" = Sigma.post[,,S1+1:S2], 
                 "k.a" = k.a[S1+1:S2], 
                 "k.sigma" = k.sigma[S1+1:S2], 
                 "B.convergence" = B.posterior[,,1:S], 
                 "A.convergence" = A.posterior[,,1:S], 
                 "Sigma.convergence" = Sigma.post[,,1:S], 
                 "k.a.convergence" = k.a[1:S], 
                 "k.sigma.convergence" = k.sigma[1:S])
  return(results)
}

```

## Artificial Data

We generate two random walks $y1$ and $y2$ with 1000 observations each
simulated from a bivariate Gaussian random walk process. We plot the raw
series and the first difference below:

```{r}
#| echo: true 
#| message: false
#| warning: false
#| code-fold: true
#| code-summary: "Artificial Data"
  set.seed(420)
  y1 <- ts(cumsum(rnorm(1000, 0, sd=1)))
  y2 <- ts(cumsum(rnorm(1000, 0, sd=1)))
  y.art <- cbind(y1,y2)
  plot(diff(y.art), 1:1000, type = "l", main = "Artificial data first difference", ylab="", xlab="")
```

Then, we use our two functions from the basic and extended model to
simulate the intercept term posterior. We show in the plot that for 10
thousand draws we get an intercept close to zero for the $A_{1,1}$ and
$A_{1,2}$ values. In addition, we get an identity matrix for the
$B_{1,1}$ and $B_{2,2}$ values. Hence, the function works as we want.

```{r}
#| echo: true 
#| message: false
#| warning: false
#| code-fold: true
#| code-summary: "Artificial Data Results"
  set.seed(420)
  y.art <- cbind(ts(cumsum(rnorm(1000, 0, sd=1))),ts(cumsum(rnorm(1000, 0, sd=1))))
  par(mfrow=c(1,2),mar=c(2,2,2,2))
  plot(diff(y.art), 1:1000, type = "l", main = "Artificial data first difference", ylab="", xlab="")
  basic.artificial <- basic.model(y.art, 1, 10000, c(), c())
  extended.artificial <- extended.model(y.art, 1, 1000, 9000)
  B.basic    <- basic.artificial$B
  A.basic    <- basic.artificial$A
  B.extended <- extended.artificial$B.convergence
  A.extended <-extended.artificial$A.convergence
  
  par(mfrow=c(4,2),mar=c(2,2,3,2))
  plot(1:10000, B.basic[1,1,], type = "l", main = expression(paste("basic " , B[11])), lwd=0.3, ylab="", xlab="", col="royalblue")
  plot(1:10000, B.basic[2,2,], type = "l", main = expression(paste("basic " , B[22])), lwd=0.3, ylab="", xlab="", col="royalblue")
  plot(1:10000, A.basic[1,1,], type = "l", main = expression(paste("basic " , A[11])), lwd=0.4, ylab="", xlab="", col="tomato")
  plot(1:10000, A.basic[1,2,], type = "l", main = expression(paste("basic " , A[12])), lwd=0.4, ylab="", xlab="", col="tomato")
  plot(1:10000, B.extended[1,1,], type = "l", main = expression(paste("extended " , B[11])), lwd=0.3, ylab="", xlab="", col="royalblue3")
  plot(1:10000, B.extended[2,2,], type = "l", main = expression(paste("extended " , B[22])), lwd=0.3, ylab="", xlab="", col="royalblue3")
  plot(1:10000, A.extended[1,1,], type = "l", main = expression(paste("extended " , A[11])), lwd=0.4, ylab="", xlab="", col="tomato3")
  plot(1:10000, A.extended[1,2,], type = "l", main = expression(paste("extended " , A[12])), lwd=0.4, ylab="", xlab="", col="tomato3")
```

Moreover, we also plot the histograms of the values $\kappa_A$ and
$\kappa_{\Sigma}$ outlining their mean value. We also add a vertical
line to define the mean parameter for all draws. In our original
function, we have set the $\kappa_{Sigma}$ initial value to 1 and the
results from 20 thousand draws indicate that the posterior median is
$\backsim0.04$. Similarly, we set the initial value of $\kappa_A$ to one
and obtain a posterior distribution with a median $\sim 14.9$.

```{r}
#| echo: true 
#| message: false
#| warning: false
#| code-fold: true
#| code-summary: "Kappas Histograms Artifical Data"
  par(mfrow=c(1,2),mar=c(2,2,2,2))
  hist(extended.artificial$k.a.convergence, breaks="Freedman-Diaconis", col='tomato3', main = expression(paste("Slope Hyperparameter ", kappa[A])), border=F, xlim=c(0,30))
  abline(v = mean(extended.artificial$k.a.convergence), col='purple4', lwd = 2)
  hist(extended.artificial$k.sigma.convergence, breaks="Freedman-Diaconis", col='royalblue', main = expression(paste("Covariance Hyperparameter ", kappa[Sigma])), border=F, xlim=c(0,0.15))
  abline(v = mean(extended.artificial$k.sigma.convergence), col='purple4', lwd = 3)
```

We plot the $\kappa$s parameters for the extended model using the
@miranda2020us dataset. The authors present the results of their impulse
response functions in percentage changes, so we assume they have
log-transformed the data. Moreover, the authors state in the online
appendix that the global factor and global risk aversion variables are
computed using first-differenced log-priced series. To support our
statement, a plot of first-differenced data provides percentage changes
on the ordinate axes, similar to what we would expect from a plot of
$log(y_{t})-log(y_{t-1})$. Hence, we will compute our parameters of
interest using first-differenced data except for our instrument variable
with 20 thousand draws in our Gibbs Sampler. Our initial $\kappa_A$
value is $\sim 0.92$ and our initial $\kappa_{\Sigma}$ value is equal
to 1. We can observe that the data changes significantly for the
hyperprior parameter for the slope, but the same cannot be said for the
variance-covariance hyperprior. After some tuning of the Inverse-Gamma2
and Gamma distribution priors for the $/kappa_A$ and the
$/kappa_[Sigma]$ we found the following optimal values:
$$ \underline{S}_A=0.5\\ 
   \underline{\nu}_A=3\\
   \underline{s}_{\Sigma}=0.01\\
   \underline{a}_{\Sigma}=1$$

```{r}
#| echo: false 
#| message: false
#| warning: false
    y = cbind(instrument,
    dgs1,
    (pce),
    (indpro),
    greaexus,
    (glbinflows),
    (bis),
    globalf,
    globalra,
    (glbcrexus),
    (usbdlev),
    (eubdlev),
    usbanksl,
    eubanksl)
  y         <- window((y), start=c(1990,2), end=c(2010,12))
```

```{r}
#| echo: true 
#| message: false
#| warning: false
#| code-fold: true
#| code-summary: "Kappas Histograms Extended Model"
  extended.results       <- extended.model(y, 4, 500, 20000)
  par(mfrow=c(1,2),mar=c(2,2,2,2))
  hist(extended.results$k.a.convergence, breaks="Freedman-Diaconis", col='tomato3', main = expression(paste("Slope Hyperparameter ", kappa[A])), border=F, xlim=c(12,22.5))
  abline(v = mean(extended.results$k.a.convergence), col='purple4', lwd = 2)
  hist(extended.results$k.sigma.convergence, breaks="Freedman-Diaconis", col='royalblue', main = expression(paste("Covariance Hyperparameter ", kappa[Sigma])), border=F)
  abline(v = mean(extended.results$k.sigma.convergence), col='purple4', lwd = 3)
```

## Impulse Response Functions

The impulse response functions are the dynamic causal effects of the
underlying shocks $u_t$ on the economic measurements $y_t$. Our model
features the instrument in the first position. We will compute impulse
response functions (IRFs) assuming that the instrument affects all the
other variables exogenously. We define an IRF as a shock of the first
variable in $u_t$ (the instrument) on the 14 variables $n$ of our model
in $Y_t$ at time $t$ until time $t+i$. In our case $i=24$ because we
want to study the IRFs for two years as the original paper authors do.
Compactly, we can describe the computations as follows: \begin{gather}
\frac{\partial y_{n.t+1}}{\partial u_{1.t}}=\theta_{nj.i}
\end{gather} @MollerWolf2021LPVARS assumption regarding the
invertibility of $\varepsilon$ is satisfied given our Cholesky
decomposition:

$$
\begin{bmatrix}
b_0^{1,1} & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
b_0^{2,1} & b_0^{2,2} & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
b_0^{3,1} & b_0^{3,2} & b_0^{3,3} & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
b_0^{4,1} & b_0^{4,2} & b_0^{4,3} & b_0^{4,4} & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
b_0^{5,1} & b_0^{5,2} & b_0^{5,3} & b_0^{5,4} & b_0^{5,5} & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
b_0^{6,1} & b_0^{6,2} & b_0^{6,3} & b_0^{6,4} & b_0^{6,5} & b_0^{6,6} & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
b_0^{7,1} & b_0^{7,2} & b_0^{7,3} & b_0^{7,4} & b_0^{7,5} & b_0^{7,6} & b_0^{7,7} & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
b_0^{8,1} & b_0^{8,2} & b_0^{8,3} & b_0^{8,4} & b_0^{8,5} & b_0^{8,6} & b_0^{8,7} & b_0^{8,8} & 0 & 0 & 0 & 0 & 0 & 0 \\
b_0^{9,1} & b_0^{9,2} & b_0^{9,3} & b_0^{9,4} & b_0^{9,5} & b_0^{9,6} & b_0^{9,7} & b_0^{9,8} & b_0^{9,9} & 0 & 0 & 0 & 0 & 0 \\
b_0^{10,1} & b_0^{10,2} & b_0^{10,3} & b_0^{10,4} & b_0^{10,5} & b_0^{10,6} & b_0^{10,7} & b_0^{10,8} & b_0^{10,9} & b_0^{10,10} & 0 & 0 & 0 & 0 \\
b_0^{11,1} & b_0^{11,2} & b_0^{11,3} & b_0^{11,4} & b_0^{11,5} & b_0^{11,6} & b_0^{11,7} & b_0^{11,8} & b_0^{11,9} & b_0^{11,10} & b_0^{11,11} & 0 & 0 & 0 \\
b_0^{12,1} & b_0^{12,2} & b_0^{12,3} & b_0^{12,4} & b_0^{12,5} & b_0^{12,6} & b_0^{12,7} & b_0^{12,8} & b_0^{12,9} & b_0^{12,10} & b_0^{12,11} & b_0^{12,12} & 0 & 0 \\
b_0^{13,1} & b_0^{13,2} & b_0^{13,3} & b_0^{13,4} & b_0^{13,5} & b_0^{13,6} & b_0^{13,7} & b_0^{13,8} & b_0^{13,9} & b_0^{13,10} & b_0^{13,11} & b_0^{13,12} & b_0^{13,13} & 0 \\
b_0^{14,1} & b_0^{14,2} & b_0^{14,3} & b_0^{14,4} & b_0^{14,5} & b_0^{14,6} & b_0^{14,7} & b_0^{14,8} & b_0^{14,9} & b_0^{14,10} & b_0^{14,11} & b_0^{14,12} & b_0^{14,13} & b_0^{14,14} \\
\end{bmatrix}
\begin{bmatrix}
\text{Instrument} \\
\text{1Y Treasury Rate} \\
\text{PCE Deflator} \\
\text{Ind. Prod.} \\
\text{G. Prod. ex US} \\
\text{G. Inflows} \\
\text{BIS EER} \\
\text{G. Factor} \\
\text{G. Risk Aversion} \\
\text{G. Cred. ex US} \\
\text{Lev. US B&D} \\
\text{Lev. EU G. Banks} \\
\text{Lev. US Banks} \\
\text{Lev. EU Banks} \\
\end{bmatrix}
$$
## IRFs Basic Model

We will outline IRFs for both our basic and extended models, computing
20 thousand draws for our estimates. @miranda2020us normalize the IRFs
so that the Treasury rate has a percentage point increase at horizon 0,
but we do not include this normalization. Because of this, we cannot
interpret the degree of shock as the authors do. From the 14 variables,
we present only ten of them. We abstain from plotting the global factor
because it is simply computed as the inverse of global risk aversion. In
addition, we do not plot US brokers & dealers' leverage and US banks'
leverage. This is because our main interests lies in spillovers of
monetary policy outside the US. The abscissa scale is in months, given
that we use monthly variables, and display the 68% and 90% density
intervals. Out of the ten IRF plots, half coincide with the authors'
results. We will briefly outline the results. -Upon realisation, the
one-year Treasury rate spikes in the first quarter, decreases in the
third quarter and then hovers around zero in the remaining lags; -we
would expect the PCE deflator to shrink, but it provides insignificant
results in the first two quarters and increases thereafter; -industrial
production shrinks in the first three quarters, and then mean-reverts
but in insignificant bands; -global real production excluding the
financial hegemon provides insignificant results in the first two lags
and then increases, similarly to global inflows; -the BIS real effective
exchange rate increases upon realisation as expected following the
monetary policy shock, decreasing from the second quarter onwards;
-global risk aversion - the proxy for global risky asset prices - spikes
and hoovers around zero from the third quarter; -global credit excluding
the US decreases as expected in the first two quarters and then
increases thereafter; -the leverage drop immediately, but contrary to
the original paper it does not mean-revert in the first quarter, and EU
banks' leverage provides insignificant results;

```{r}
#| echo: true 
#| message: false
#| warning: false
#| code-fold: true
#| code-summary: "IRFs Basic Model"
  # Impulse response functions basic model
  # Forecast Error Variance Decomposition
  ###########################################################
  basic.results       <- basic.model(y, 12, 20000, c(1990,2), c(2010,12))
  N=ncol(y)
  h=24
  p=12
  S=20000 #S2 draws of the extended model
  IRF.posterior     = array(NA,c(N,N,h+1,S))
  IRF.inf.posterior = array(NA,c(N,N,S))
  FEVD.posterior    = array(NA,c(N,N,h+1,S))
  J                 = cbind(diag(N),matrix(0,N,N*(p-1))) #[I_N,0_{NxN(p-1)}]
  
  for (s in 1:S){
  A.bold          = rbind(t(basic.results$A[2:(1+N*p),,s]),cbind(diag(N*(p-1)),matrix(0,N*(p-1),N)))  #rbind([A1...AP], cbind(I_{N(p-1)}, 0_{N(p-1)xN}))
  IRF.inf.posterior[,,s]          = J %*% solve(diag(N*p)-A.bold) %*% t(J) #J*(I-A)^-1*J'
  A.bold.power    = A.bold
  for (i in 1:(h+1)){
    if (i==1){
      IRF.posterior[,,i,s]        = basic.results$B[,,s] #first value is first IRF
    } else {
      IRF.posterior[,,i,s]        = J %*% A.bold.power %*% t(J) %*%basic.results$B[,,s] #J*A*J'*B.posterior
      A.bold.power                = A.bold.power %*% A.bold
    }
    for (n in 1:N){
    for (nn in 1:N){
    FEVD.posterior[n,nn,i,s]  = sum(IRF.posterior[n,nn,1:i,s]^2)
     }
    }
    FEVD.posterior[,,i,s]         = diag(1/apply(FEVD.posterior[,,i,s],1,sum))%*%FEVD.posterior[,,i,s]
  }
}
  FEVD.posterior    = 100*FEVD.posterior

  #colours for IRFs
  ############################################################
  mcxs2  = "#379683"
  mcxs3  = "#5CDB95"
  mcxs4  = "#8EE4AF"
  mcxs3.rgb   = col2rgb(mcxs3)
  mcxs3.shade1= rgb(mcxs3.rgb[1],mcxs3.rgb[2],mcxs3.rgb[3], alpha=120, maxColorValue=255)
  mcxs4.rgb   = col2rgb(mcxs4)
  mcxs4.shade1= rgb(mcxs4.rgb[1],mcxs4.rgb[2],mcxs4.rgb[3], alpha=120, maxColorValue=255)


  # plot IRFs basic model
  ############################################################
  #load("irf-instrument-fevd.RData")
  IRF.posterior.inst = IRF.posterior[,1,,] #takes the instruments IRFs
  IRFs.k1           = apply(IRF.posterior.inst,1:2,mean) #mean of the S IRFs
  IRFs.inf.k1       = apply(IRF.posterior.inst,1,mean)
  rownames(IRFs.k1) = y.names

  IRFs.k1.hdi    = apply(IRF.posterior.inst,1:2,hdi, credMass=0.68)
  IRFs.k1.hdi2    = apply(IRF.posterior.inst,1:2,hdi, credMass=0.9)
  hh          = 1:(h+1)
  #pdf(file="FF-irf-mps1.pdf", height=9, width=12)
  par(mfrow=c(5,2), mar=c(2, 2, 2, 1.5),cex.axis=0.8, cex.lab=0.8, cex.main=0.9)
  IRFs.wanted      <- c(2,3,4,5,6,7,9,10,12,14)
  for (n in 1:14){
    if (n %in% IRFs.wanted) {
  ylims     = range(IRFs.k1[n,hh],IRFs.k1.hdi[,n,hh])
  plot(hh,IRFs.k1[n,hh], type="l", ylim=ylims, axes=FALSE, xlab="", ylab="", main =rownames(IRFs.k1)[n])
  lines(hh, IRFs.k1[n,hh], lwd=2, col=mcxs2)
  axis(1,c(4,8,12,16,20,24),c("4","8","12","16","20","24"))
  axis(2,c(ylims[1],0,ylims[2]),round(c(ylims[1],0,ylims[2]),3))
  polygon(c(hh,(h+1):1), c(IRFs.k1.hdi[1,n,hh],IRFs.k1.hdi[2,n,(h+1):1]), col=mcxs3.shade1,border=mcxs3.shade1)
  polygon(c(hh,(h+1):1), c(IRFs.k1.hdi2[1,n,hh],IRFs.k1.hdi2[2,n,(h+1):1]), col=mcxs4.shade1,border=mcxs4.shade1)
  abline(h=0)
    }
  }
  
```

### FEVD Basic Model

Moreover, we include forecast error variance decomposition (FEVD) of two
variables, i.e. EU Global banks' leverage, and EU big but systemic banks
leverage. FEVD provide the information of how much each variable
contributes to the information of the other variables' forecast
variability at each horizon. We can observe that the FEVD for Global
banks the other variables account significantly for the aforementioned
information at further horizons. As a matter of fact, the information
provided by the variable of interest contributes for only 13% at the
last horizon. The larger contributors at the last horizon are the
leverage of US and EU global but not systematically important banks
(\>40%).

```{r}
#| echo: true 
#| message: false
#| warning: false
#| code-fold: true
#| code-summary: "FEVD Basic Model for EU Global banks' leverage"
  # colours for FEVDs
  ############################################################
  colours = c("deepskyblue1","deepskyblue2","deepskyblue","deepskyblue3","deepskyblue4","dodgerblue",
           "dodgerblue1","dodgerblue2","maroon1","maroon","maroon2","magenta","maroon3","maroon4")

  # plot FEVDs for EU Global Banks' leverage
  ############################################################
  fevd.eubdlev  = apply(FEVD.posterior[12,,,],1:2,mean)
  fevd.eubdlev  = rbind(rep(0,h+1),apply(fevd.eubdlev,2,cumsum))

  par(mfrow=c(1,1), mar=rep(4,4),cex.axis=1, cex.lab=0.8)
  plot(hh,fevd.eubdlev[1,], type="n", ylim=c(0,100), axes=FALSE, xlab="", ylab="", main = "FEVD EU Global Banks leverage Basic Model")
  axis(1,hh,c("","","","4","","","","8","","","","12","","","","16","","","","20","","","","24",""),cex.axis=0.8)
  axis(2,c(0,50,100),c("","EUBDLEV",""), cex.axis=0.9)
  for (n in 1:N){
   polygon(c(hh,(h+1):1), c(fevd.eubdlev[n,hh],fevd.eubdlev[n+1,(h+1):1]), col=colours[n],border=colours[n])
  }  
  axis(4,(0.5*(fevd.eubdlev[1:14,25]+fevd.eubdlev[2:15,25]))[c(2,14)], c("Treasury rate","EUBANKSL"),cex.axis=0.6)
```

In addition, similar conclusions can be drawn to the FEVD of EU big but
systematic banks' leverage. The information provided by the variable of
interest contributes for 12% at the last horizon. Other larger
contributors at the last horizon are Leverage for EU global banks and US
banks.

```{r}
#| echo: true 
#| message: false
#| warning: false
#| code-fold: true
#| code-summary: "FEVD Basic Model for EU banks' leverage"
  # plot FEVDs for EU banks' leverage
  ############################################################
  fevd.eulev  = apply(FEVD.posterior[14,,,],1:2,mean) #real gdp
  fevd.eulev  = rbind(rep(0,h+1),apply(fevd.eulev,2,cumsum))

  #pdf(file="fevd-inst.pdf", height=7, width=12)
  par(mar=rep(4,4),cex.axis=1, cex.lab=0.8)
  plot(hh,fevd.eulev[1,], type="n", ylim=c(0,100), axes=FALSE, xlab="", ylab="",main="FEVD EU Banks leverage Basic Model")
  axis(1,hh,c("","","","4","","","","8","","","","12","","","","16","","","","20","","","","24",""),cex.axis=0.8)
  axis(2,c(0,50,100),c("","EUBANKSL",""),cex.axis=0.9)
  for (n in 1:N){
   polygon(c(hh,(h+1):1), c(fevd.eulev[n,hh],fevd.eulev[n+1,(h+1):1]), col=colours[n],border=colours[n])
  }
  axis(4,(0.5*(fevd.eulev[1:14,25]+fevd.eulev[2:15,25]))[c(2,13)], c("Treasury rate","Leverage US Banks"),cex.axis=0.6)
```

### IRFs Extended Model

Regarding the extended model, the IRFs are not in line with previous
results or expectations. This is except for EU global banks' leverage
and the BIS effective exchange rate. We have worked on tuning different
parameters for the $\kappa_A$ and $\kappa_{\Sigma}$ hyperpriors, but
unfortunately we still have to identify the drivers of these results.
The IRFs of these variables provide insightful results only in the first
4-8 lags, but at further points the outcome is insignificant. One
solution could be to implement a grid-search of different parameters for
the hyperpriors prior parameters in the Gamma and Inverse-Gamma2
distributions, but time and computation power constraints prevent us
from performing such exercise.

```{r}
#| echo: true 
#| message: false
#| warning: false
#| code-fold: true
#| code-summary: "IRFs Extended Model"
  # Extended model Impulse response functions
  # Forecast Error Variance Decomposition
  ###########################################################
  p=4
  IRF.posterior2     = array(NA,c(N,N,h+1,S))
  IRF.inf.posterior2 = array(NA,c(N,N,S))
  FEVD.posterior2    = array(NA,c(N,N,h+1,S))
  J2                 = cbind(diag(N),matrix(0,N,N*(p-1))) #[I_N,0_{NxN(p-1)}]
  
  for (s in 1:S){
  A.bold2          = rbind(t(extended.results$A[2:(1+N*p),,s]),cbind(diag(N*(p-1)),matrix(0,N*(p-1),N)))  #rbind([A1...AP], cbind(I_{N(p-1)}, 0_{N(p-1)xN}))
  IRF.inf.posterior2[,,s]          = J2 %*% solve(diag(N*p)-A.bold2) %*% t(J2) #J*(I-A)^-1*J'
  A.bold.power2    = A.bold2
  for (i in 1:(h+1)){
    if (i==1){
      IRF.posterior2[,,i,s]        = extended.results$B[,,s] #first value is first IRF
    } else {
      IRF.posterior2[,,i,s]        = J2 %*% A.bold.power2 %*% t(J2) %*%extended.results$B[,,s] #J*A*J'*B.posterior
      A.bold.power2                = A.bold.power2 %*% A.bold2
    }
    for (n in 1:N){
    for (nn in 1:N){
    FEVD.posterior2[n,nn,i,s]  = sum(IRF.posterior2[n,nn,1:i,s]^2)
     }
    }
    FEVD.posterior2[,,i,s]         = diag(1/apply(FEVD.posterior2[,,i,s],1,sum))%*%FEVD.posterior2[,,i,s]
  }
}
  FEVD.posterior2    = 100*FEVD.posterior2

  #colours for IRFs
  ############################################################
  mcxs2  = "#379683"
  mcxs3  = "#5CDB95"
  mcxs4  = "#8EE4AF"
  mcxs3.rgb   = col2rgb(mcxs3)
  mcxs3.shade1= rgb(mcxs3.rgb[1],mcxs3.rgb[2],mcxs3.rgb[3], alpha=120, maxColorValue=255)
  mcxs4.rgb   = col2rgb(mcxs4)
  mcxs4.shade1= rgb(mcxs4.rgb[1],mcxs4.rgb[2],mcxs4.rgb[3], alpha=120, maxColorValue=255)


  # plot IRFs extended model
  ############################################################
  IRF.posterior.inst2 = IRF.posterior2[,1,,] #takes the instruments IRFs
  IRFs.k12           = apply(IRF.posterior.inst2,1:2,mean) #mean of the S IRFs
  IRFs.inf.k12       = apply(IRF.posterior.inst2,1,mean)
  rownames(IRFs.k12) = y.names

  IRFs.k1.hdi3    = apply(IRF.posterior.inst2,1:2,hdi, credMass=0.68)
  IRFs.k1.hdi4    = apply(IRF.posterior.inst2,1:2,hdi, credMass=0.9)
  hh          = 1:(h+1)
  par(mfrow=c(5,2), mar=c(2, 2, 2, 1.5),cex.axis=0.8, cex.lab=0.8, cex.main=0.9)
  for (n in 1:14){
    if (n %in% IRFs.wanted) {
  ylims     = range(IRFs.k12[n,hh],IRFs.k1.hdi3[,n,hh])
  plot(hh,IRFs.k12[n,hh], type="l", ylim=ylims, axes=FALSE, xlab="", ylab="", main =rownames(IRFs.k12)[n])
  lines(hh, IRFs.k12[n,hh], lwd=2, col=mcxs2)
  axis(1,c(4,8,12,16,20,24),c("4","8","12","16","20","24"))
  axis(2,c(ylims[1],0,ylims[2]),round(c(ylims[1],0,ylims[2]),3))
  polygon(c(hh,(h+1):1), c(IRFs.k1.hdi3[1,n,hh],IRFs.k1.hdi3[2,n,(h+1):1]), col=mcxs3.shade1,border=mcxs3.shade1)
  polygon(c(hh,(h+1):1), c(IRFs.k1.hdi4[1,n,hh],IRFs.k1.hdi4[2,n,(h+1):1]), col=mcxs4.shade1,border=mcxs4.shade1)
  abline(h=0)
    }
  }
```

### FEVD Extended Model

We also plot the same FEVD of the same variables of the previous
section. Differenyly from IRFs, the extended model FEVDs are almost
identical to the ones of the basic model.

```{r}
#| echo: true 
#| message: false
#| warning: false
#| code-fold: true
#| code-summary: "FEVD Extended Model for EU Global banks' leverage"
  # plot FEVDs for EU Global Banks' leverage
  ############################################################
  fevd.eubdlev2  = apply(FEVD.posterior2[12,,,],1:2,mean)
  fevd.eubdlev2  = rbind(rep(0,h+1),apply(fevd.eubdlev2,2,cumsum))
  par(mfrow=c(1,1), mar=c(4, 4, 4, 4),cex.axis=1, cex.lab=0.8)
  plot(hh,fevd.eubdlev2[1,], type="n", ylim=c(0,100), axes=FALSE, xlab="", ylab="", main = "FEVD EU Global Banks leverage Extended Model")
   axis(1,hh,c("","","","4","","","","8","","","","12","","","","16","","","","20","","","","24",""),cex.axis=0.8)
  axis(2,c(0,50,100),c("","EUBDLEV ",""), cex.axis=0.9)
  for (n in 1:N){
   polygon(c(hh,(h+1):1), c(fevd.eubdlev2[n,hh],fevd.eubdlev2[n+1,(h+1):1]), col=colours[n],border=colours[n])
  }  
  axis(4,(0.5*(fevd.eubdlev2[1:14,25]+fevd.eubdlev2[2:15,25]))[c(2,14)], c("Treasury rate","EUBANKSL"),cex.axis=0.6)
```

```{r}
#| echo: true 
#| message: false
#| warning: false
#| code-fold: true
#| code-summary: "FEVD Extended Model for EU banks' leverage"
  # plot FEVDs for EU banks' leverage
  ############################################################
  fevd.eulev2  = apply(FEVD.posterior2[14,,,],1:2,mean) #real gdp
  fevd.eulev2  = rbind(rep(0,h+1),apply(fevd.eulev2,2,cumsum))

  par(mar=rep(4,4),cex.axis=1, cex.lab=0.8)
  plot(hh,fevd.eulev2[1,], type="n", ylim=c(0,100), axes=FALSE, xlab="", ylab="",main="FEVD EU Banks leverage Extended Model")
  axis(1,hh,c("","","","4","","","","8","","","","12","","","","16","","","","20","","","","24",""),cex.axis=0.8)
  axis(2,c(0,50,100),c("","EUBANKSL",""),cex.axis=0.9)
  for (n in 1:N){
   polygon(c(hh,(h+1):1), c(fevd.eulev2[n,hh],fevd.eulev2[n+1,(h+1):1]), col=colours[n],border=colours[n])
  }  
  axis(4,(0.5*(fevd.eulev2[1:14,25]+fevd.eulev2[2:15,25]))[c(2,13)], c("Treasury rate","Leverage US Banks"),cex.axis=0.6)
```

# References
:::

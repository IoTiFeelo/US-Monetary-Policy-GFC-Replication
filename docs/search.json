[
  {
    "objectID": "index.html#global-factor-in-risky-asset-prices",
    "href": "index.html#global-factor-in-risky-asset-prices",
    "title": "A Proxy-SVAR Reproduction Model",
    "section": "Global Factor in Risky Asset Prices",
    "text": "Global Factor in Risky Asset Prices\nMiranda-Agrippino and Rey in the first part of the paper estimate a global factor to proxy the movement of world risky asset prices. They do so by collecting 858 prices of different risky assets traded in North America, Latin America, Europe, Asia Pacific, and Australia, from 1990 to 2012. Their method is to pick a representative market index (i.e. S&P 500) for each market at the end of 2012, including all of its components, selecting prices that allow them to cover at least 80% of cross sectional observations by 1990 and 95% in 1995. They do so to avoid over-representation of each category. With this global factor, they can explain over 20% of global risky asset price volatility in their time span. Given the small time frame and VAR analysis limitations, they estimate a global factor with commodities from the U.S., Europe, and Japan, spanning back to 1975. This factor covers 60% of the volatility in this period. The appendix of the paper provides detailed information on this VAR estimation. To provide more intuition on this factor, the authors correlate it with some indexes of implied volatility such as the VIX, outlining its co-movement with common measures of market variation (in this case a negative correlation). The global factor will be used later in the impulse-response section.\n\n\n\n\n#plot of global factors with VIX\nggplot() + \n  geom_line(data = gf, aes(x = `...1`, y = `GLOBAL FACTOR 1975-2010`, linetype = \"GF 1975-2010\"), color =\"blue\", na.rm = TRUE, linewidth=1.2) + \n  geom_line(data = gf, aes(x = `...1`, y = `GLOBAL FACTOR 1990-2012`, linetype = \"GF 1990-2012\"), color=\"purple\", na.rm = TRUE, linewidth=1.2) +\n  geom_line(data = data, aes(x = `LABEL`, y = VIX, linetype = \"VIX\"), color=\"black\", na.rm = TRUE, linewidth=1.2) +\n  scale_linetype_manual(values = c(\"solid\", \"solid\", \"solid\"), \n                        labels = c(\"VIX\", \"GF 1975-2010\", \"GF 1990-2012\"),\n                        guide = guide_legend(override.aes = list(color = c(\"black\", \"blue\", \"purple\")))) +\n  labs(x = \"\", y = \"\", title = \"Global Factor for Risky Asset Prices\") +\n  theme(plot.title = element_text(size = 20)) +\n  theme(legend.text = element_text(size = 20), legend.title = element_text(size = 0), \n        legend.position = \"bottom\", plot.title = element_text(hjust = 0.5), \n        panel.border = element_blank(), panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(), axis.line = element_line(colour = \"black\"),\n        panel.background = element_rect(fill = \"white\"), plot.background = element_rect(fill = \"white\"), \n        axis.text = element_text(size = 14))"
  },
  {
    "objectID": "index.html#proxy-var-analysis-with-rich-information-bayesian-var",
    "href": "index.html#proxy-var-analysis-with-rich-information-bayesian-var",
    "title": "A Proxy-SVAR Reproduction Model",
    "section": "Proxy-VAR Analysis with Rich-Information Bayesian VAR",
    "text": "Proxy-VAR Analysis with Rich-Information Bayesian VAR\nIn this project, I will avoid the computation of the global factor, because it is out of scope of the Macroeconomerics subject. Instead, I will concentrate on the Bayesian VAR analysis of Miranda-Agrippino and Rey. A main reason why the authors studied the monetary effects of U.S. interest rate changes is that the dollar is the currency of global banking. A change in FED monetary policy affects banks’ borrowing capacity, the pricing of dollar denominated assets, and cross-border capital flows. In order to isolate its effects, the two scholars identify U.S. monetary policy shocks by exploiting 30-min price revisions around Federal Open Market Committee (FOMC) announcements in the fourth federal funds futures contracts (FF4). The intuition is that these futures have an average maturity of three months, and they can predict revisions of market expectations about future monetary policy one-quarter in advance. This assumption holds only if market participants can distinguish between the systematic component of policy and any observable policy action. Moreover, with asymmetrical information, the FF4 revisions contain information about the influence of economic factors relevant to U.S. monetary policy. Policy announcements provide this information implicitly."
  },
  {
    "objectID": "index.html#the-data",
    "href": "index.html#the-data",
    "title": "A Proxy-SVAR Reproduction Model",
    "section": "The Data",
    "text": "The Data\nI download the data directly from the website of Miranda-Agrippino. The two authors study the consequences of a 1% increase in the U.S. monetary policy considering:\n-a domestic VAR with the effects on domestic financial markets and macroeconomic aggregates in the United States;\n-a global VAR with the effects on global asset markets, global domestic credit and international capital flows;\n-a “floaters” VAR to study if a fixed or pegged exchange rate affects the global contraction.\nI will study the global specifications, and I will include the following variables:"
  },
  {
    "objectID": "index.html#bvar-framework",
    "href": "index.html#bvar-framework",
    "title": "A Proxy-SVAR Reproduction Model",
    "section": "BVAR Framework",
    "text": "BVAR Framework\nFrom Herwartz (2021) I define the model as: \\[\ny_t=B_0B_1y_{t-1}+...+B_0B_py_{t-p}+B_0\\varepsilon_t, t=1,..,T \\] \\[y_t=A_1y_{t-1}+...+A_py_{t-p}+u_t\\] where \\(A_j, : \\left \\{j=1,2,\\dots,p\\right \\}\\) where \\(A(L)=B_0B(L)\\) are a K x K coefficient matrices, and L is the lag operator, and \\(u_t\\) in the first reduced form is serially uncorrelated with zero mean and positive definite (non-diagonal) covariance matrix \\(\\Sigma_u.\\) The structural shocks \\(\\varepsilon_t\\) in the second reduced form assumed to be mutually uncorrelated and normalised to have unit variance. \\(\\Xi\\left(\\varepsilon_t \\varepsilon_t' \\right)=I_K\\). Structural shocks are mapped to the reduced-form system through a K x K non-singular matrix \\(B_0\\), such that \\(B_0^{-1}B_0^{-1'}=\\Sigma_u\\). For simplicity, the process is assumed to be causal and \\(detA(z)=det(I_k-\\sum_{j=1}^{p}A_jz^{j})\\neq 0\\) for \\(\\left| z \\right|\\le 1\\). This ensures that the process has a Wold moving average MA representation. Moreover,\n\\[\\begin{align*}\ny_t=\\mu+\\sum_{i=0}^{\\infty}\\Phi_iu_{t-i}=\\mu+\\sum_{i=0}^{\\infty}\\Phi_iB_0\\varepsilon_{t-i}=\n\\mu+\\sum_{i=0}^{\\infty}\\Theta_i\\varepsilon_{t-i}.\n\\end{align*}\\]\nWith: \\[\\begin{gather}\n\\mu=A(1)^{-1}\\nu, ::: \\Phi_0=I_K, \\ \\Phi_i=\\sum_{j=1}^{i}A_j\\Phi_{i-j}, ::: A_j=0 :for: j>p.\n\\end{gather}\\] The second to last MA representation is of particular importance because the structural MA coefficients \\(\\Theta_i=\\Phi_iB_0\\) cannot be recovered without a proper identification. I will briefly outline the Proxy SVAR approach.\nLet \\(z_t\\) be an external instrument to identify the structural shock of interest \\(\\varepsilon_{kt}, k\\:\\epsilon \\: \\left \\{1,\\dots, K\\right \\}\\). \\(z_t\\) has to satisfy the relevant condition \\(\\Xi(\\varepsilon_{kt}z_t)=\\phi \\neq 0\\) and the exogeneity condition \\(\\Xi(\\varepsilon{lt}z_t)=0, \\forall l\\:\\epsilon\\left \\{1,\\dots, K\\right \\}\\setminus\\left \\{k\\right \\}\\).\nFrom these conditions, it follows that the population covariance between the instrument and VAR residuals obtain the k-th column if \\(B_0\\), denoted by \\(B_{0k}\\).\n\\[\n\\Xi(u_tz_t) = B_{0,k} \\\\\n\\Xi(\\varepsilon{kt}z_t)=\\phi B_{0,k}\n\\]\nMoreover, let \\(\\Pi\\) denotes the \\(1\\)x\\(K\\) coefficient vector from the regression of the instrument on the residual vector \\(u_t\\) gives the shock \\(\\varepsilon_{kt}\\) up to a scale \\(\\phi\\).\n\\(\\Pi u_t=\\Xi(z_tu^{'}_t)\\Sigma_{u}^{-1}u_t=\\phi B^{'}_{0,k}\\left [B_0B_0^{'} \\right ]u_t=\\phi e^{'}_t\\varepsilon_{kt}\\)\nPlagborg-Moller (2021), exploiting their result that Local Projections and VAR impulse response function are equal up to a constant of proportionality, show that proxy SVARS impulse responses can be computed putting the instrument in the first row of the data vector \\(y_t\\) in a SVAR framework. This result follows from the invertibility of \\(\\varepsilon\\) and two assumptions:\n-the data \\(y_t\\) is covariance-stationary;\n-the data \\(y_t\\) is a jointly Gaussian vector time series.\nIn our Bayesian approach these requirements are met when we define the distributions of our error terms. I will outline them in the next section."
  },
  {
    "objectID": "index.html#basic-model",
    "href": "index.html#basic-model",
    "title": "A Proxy-SVAR Reproduction Model",
    "section": "Basic Model",
    "text": "Basic Model\nI specify my model to follow a matrix-variate normal distribution \\[\\begin{gather}\nY = XA + E \\\\\n\\\\ E|X \\sim MN_{T \\times N}(0_{T \\times N},\\Sigma,I_T)\n\\end{gather}\\] Given that the function Y is a linear combination of the error terms E, we can specify \\[\\begin{gather}\nY|X,A,\\sim MN_{T \\times N}(XA,\\Sigma,I_T)\n\\end{gather}\\] Hence, the Likelihood function follows a Matrix-Variate-Normal form: \\[\\begin{gather}\nL(A,\\Sigma|Y,X) \\propto det(\\Sigma)^{-\\frac{T}{2}} exp \\left\\{-\\frac{1}{2} tr \\left[ \\Sigma^{-1}(Y-XA)'(Y-XA) \\right] \\right\\} \\\\\n\\\\ \\propto det(\\Sigma)^{-\\frac{T}{2}} exp \\left\\{-\\frac{1}{2} tr \\left[ \\Sigma^{-1}(A-\\hat{A})'X'X(A-\\hat{A}) \\right] \\right\\} exp \\left\\{-\\frac{1}{2} tr \\left[\\Sigma^{-1}(Y-X \\hat{A})'(Y-X \\hat{A}) \\right] \\right\\} \\\\\n\\end{gather}\\] where from Maximum Likelihood Estimation we have obtained \\[\\begin{gather}\n\\hat{A} = (X'X)^{-1}X'Y \\\\\n\\\\ \\hat{\\Sigma} = \\frac{1}{T} (Y-X \\hat{A})'(Y-X \\hat{A})\n\\end{gather}\\] In the basic model, our prior follows a natural-conjugate prior distribution of the same form: \\[\\begin{gather}\np(A,\\Sigma) = p(A|\\Sigma) p(\\Sigma) \\\\\n\\\\ A|\\Sigma \\sim MN_{K \\times N} (\\underline{A}, \\Sigma , \\underline{V}) \\\\\n\\\\ \\Sigma \\sim IW_{N}(\\underline{S},\\underline{\\nu})\n\\end{gather}\\]\nWith parameters: \\[\\begin{gather}\n\\underline{A} = [0_{N \\times 1} \\quad I_N \\quad 0_{N \\times (p-1)N}]' \\\\\n\\\\ Var[vec(A)] = \\Sigma \\otimes  \\underline{V} \\\\\n\\\\ \\underline{V} = diag([\\kappa_2 \\quad \\kappa_1 (p^{-2} \\otimes I_N)]) \\\\\n\\\\ p = [1,2,...p]\n\\end{gather}\\] The parameter \\(\\kappa_2\\) and \\(\\kappa_1\\) specify respectively the prior of the overall shrinkage level for the constant term and for the autoregressive slopes. I specify \\(\\kappa_2=1\\) and \\(\\kappa_1=0.02\\) based on the fact that you should shrink the constant term much less than the autoregressive parameters. Furthermore, the full conditional posterior is: \\[\\begin{gather}\np(A,\\Sigma|Y,X) = p(A|Y,X,\\Sigma)p(\\Sigma|Y,X) \\\\\n\\\\ p(A|Y,X,\\Sigma) = MN_{K \\times N}(\\bar{A}, \\Sigma, \\bar{V}) \\\\\n\\\\ p(\\Sigma | Y, X) = IW_N(\\bar{S},\\bar{\\nu})\n\\end{gather}\\]\nWe can derive the full conditional posterior: \\[\\begin{gather}\nP(A,\\Sigma|Y,X) \\propto L(A,\\Sigma|Y,X)p(A,\\Sigma) \\\\\n\\\\ \\propto L(A,\\Sigma|Y,X)p(A|\\Sigma)p(\\Sigma) \\\\\n\\\\ det(\\Sigma)^{-\\frac{T}{2}} \\times exp \\left\\{-\\frac{1}{2} tr \\left[ \\Sigma^{-1}(A-\\hat{A})' X'X (A-\\hat{A})\\right] \\right\\} \\\\\n\\\\ \\times exp\\left\\{-\\frac{1}{2}tr \\left[ \\Sigma^{-1}(Y-X\\hat{A})'(Y-X\\hat{A}) \\right] \\right\\} \\\\\n\\\\ \\times det(\\Sigma)^{-\\frac{N+K+\\underline{\\nu}+1}{2}} \\\\\n\\\\ \\times exp\\left\\{-\\frac{1}{2}tr \\left[ \\Sigma^{-1}(A-\\underline{A})'\\underline{V}^{-1}(A-\\underline{A}) \\right] \\right\\} \\\\\n\\\\ \\times exp \\left\\{ -\\frac{1}{2} tr \\left[ \\Sigma^{-1} \\underline{S} \\right] \\right\\}\n\\end{gather}\\]\nAfter some calculations:\n\\[\\begin{gather}\\\\\np(A,\\Sigma|Y,X) \\propto \\\\\n\\\\ det{(\\Sigma)}^{-\\frac{T+N+K+ \\underline{\\nu}\n+1}{2}} \\times exp\\left\\{ -\\frac{1}{2} tr \\left[ \\Sigma^{-1} \\left[ (A-\\bar{A})^{'} \\bar{V}^{-1} (A-\\bar{A})+\\underline{S} +Y^{'}Y + \\underline{A}^{'} \\underline{V}^{-1}\\underline{A} -\\bar{A}^{'} \\bar{V}^{-1}\\bar{A}\\right]\\right]\\right\\}\n\\end{gather}\\]\nwhere the full conditional posterior has the same natural-conjugate form of our prior:\n\\[\\begin{gather}\np(A,\\Sigma|Y,X) = p(A|Y,X,\\Sigma)p(\\Sigma|Y,X) \\\\\n\\\\ p(A|Y,X,\\Sigma) = MN_{K \\times N}(\\bar{A}, \\Sigma, \\bar{V}) \\\\\n\\\\ p(\\Sigma | Y, X) = IW_N(\\bar{S},\\bar{\\nu})\\end{gather}\\]\nwith posterior parameters:\n\\[\\begin{gather}\n\\bar{V} = (X^{'}X+ \\underline{V}^{-1})^{-1} \\\\\n\\\\ \\bar{A} = \\bar{V}(X^{'}Y+\\underline{V}^{-1} \\underline{A}) \\\\\n\\\\ \\bar{\\nu} = T + \\underline{\\nu} \\\\\n\\\\ \\bar{S} = \\underline{S} + Y^{'}Y +  \\underline{A}^{'}\\underline{V}^{-1}\\underline{A} - \\bar{A}^{'}\\bar{V}^{-1}\\bar{A}\n\\end{gather}\\]\nIn order to compute our posterior parameters in R, we first specify values for our priors, then we calculate the posteriors and draw \\(A\\) and \\(\\Sigma\\) respectively from a Matrix-Variate-normal and Inverse Wishart distributions. At this point, we can obtain our structural parameters through a cholesky decomposition of our matrix $ $, namely with $ {-1}=(B_0B_0{’}) $.\n\n  # setup function for my analysis\n  basic.model <- function(bigy, p, S, start, end){\n  ############################################################\n  N       = ncol(bigy)\n  K       = 1+N*p\n  ############################################################\n  Y       = bigy[(p+1):nrow(bigy),]\n  X       = matrix(1,nrow(Y),1)\n  for (i in 1:p){\n    X     = cbind(X,bigy[(p+1):nrow(bigy)-i,])\n  }\n  \n  A.prior     = matrix(0,K,N)\n  A.prior[2:(N+1),] <- diag(c(0,rep(1,N-1))) #0 for the instrument (stationary variable)\n  V.prior     = (diag(c(1,0.02*((1:p)^(-2))%x%rep(1,N)))) #1 is kappa.2, 0.02 is kappa.1\n  S.prior     = diag(N)\n  nu.prior    = N+1\n  \n  # normal-inverse Wishart posterior parameters\n  ############################################################\n  V.bar.inv   = t(X)%*%X + diag(1/diag(V.prior)) #X'X+diag(V^-1)\n  V.bar       = solve(V.bar.inv) #inv(X'X+diag(V^-1))\n  A.bar       = V.bar%*%(t(X)%*%Y + diag(1/diag(V.prior))%*%A.prior) #V.bar(X'X+diag((V.prior)^-1(A.prior)))\n  nu.bar      = nrow(Y) + nu.prior \n  S.bar       = S.prior + t(Y)%*%Y + t(A.prior)%*%diag(1/diag(V.prior))%*%A.prior - t(A.bar)%*%V.bar.inv%*%A.bar\n  #S.prior+Y'Y+(A.prior)'*diag(diag((V.prior)^-1))*A.prior-A.bar'*(X'X+diag(V^-1))*A.bar\n  S.bar.inv   = solve(S.bar)\n  \n  # posterior draws\n  ############################################################\n  Sigma.posterior   = rWishart(S, df=nu.bar, Sigma=S.bar.inv)\n  Sigma.posterior   = apply(Sigma.posterior,3,solve)\n  Sigma.posterior   = array(Sigma.posterior,c(N,N,S))\n  A.posterior       = array(rnorm(prod(c(dim(A.bar),S))),c(dim(A.bar),S)) #3 dimensional, S repetitions of rnorm\n  B.posterior       = array(NA,c(N,N,S))\n  L                 = t(chol(V.bar))\n  for (s in 1:S){\n    cholSigma.s     = chol(Sigma.posterior[,,s])\n    B.posterior[,,s]= t(cholSigma.s)\n    A.posterior[,,s]= A.bar + L%*%A.posterior[,,s]%*%cholSigma.s\n}\n  #return the parameters of interest\n  results = list(\"B\" = B.posterior, \"A\" = A.posterior )\n  return(results)\n  }\n\n\n\n\n##Extended Model\nIn the extended model, I set hyperprior parameters for the autoregressive parameter \\(\\kappa_A\\) to follow an Inverse Gamma 2 distribution \\(IG2(\\underline{S}_{\\kappa},\\underline{\\nu}_{\\kappa})\\) and \\(\\kappa_{\\Sigma}\\) hyperprior parameter for the variance-covariance matrix to follow a Gamma Distribution \\(G(\\underline{S}_{\\Sigma},\\underline{a}_{\\Sigma})\\). I will define the full conditional posterior of the hyperparameter \\(\\kappa_A\\) first.\n\\[\\begin{gather}\np(\\kappa_a | A,\\Sigma, Y,X) \\propto L(Y|X,A,\\Sigma) \\times p(\\kappa_a) \\times p(A|\\Sigma,\\kappa_a) \\times p(\\Sigma|\\kappa_a) \\\\\n\\\\ \\propto p(\\kappa_a) \\times p(A|\\Sigma,\\kappa_a) \\\\\n\\\\ \\propto (\\kappa_a)^{-\\frac{\\underline{\\nu_a}+2}{2}}\nexp\\left\\{ -\\frac{1}{2} \\frac{\\underline{S_a}}{\\kappa_a} \\right\\}  \\times exp\\left\\{-\\frac{1}{2}tr\\left[\\Sigma^{-1}(A-\\underline{A_{\\kappa}})^{'} \\frac{1}{\\kappa}(\\underline{V_{\\kappa}})^{-1} (A-\\underline{A})\\right]\\right\\} \\times det(\\kappa_a \\underline{V})^{-\\frac{N}{2}} \\\\\n\\\\ \\propto (\\kappa_a)^{-\\frac{\\underline{\\nu_a}_{\\kappa_a}+2+NK}{2}} exp \\left\\{-\\frac{1}{2} \\frac{1}{\\kappa_a} \\left[\\underline{S_a}_{\\kappa_a} + tr \\left[\\Sigma^{-1}(A-\\underline{A})^{'}\\underline{V}^{-1}(A-\\underline{A}) \\right]\\right] \\right\\}\n\\end{gather}\\]\nwhere we can recognise the kernel of an Inverse Gamma 2 Distribution with\n\\[\\begin{gather}\n\\bar{S}_a = \\underline{S}_a+tr \\left[ \\Sigma^{-1} (A-\\underline{A})^{'} \\underline{V}^{-1} (A-\\underline{A})\\right] \\\\\n\\\\ \\bar{\\nu}_a = \\underline{\\nu}_a+NK\n\\end{gather}\\]\nIn addition, I derive similarly the full conditional posterior of the hyperparameter \\(\\kappa_{\\Sigma}\\). \\[\\begin{gather}\np(\\kappa_{\\Sigma} | A,\\Sigma, Y,X) \\propto\np(\\kappa_{\\Sigma} | A,\\Sigma, Y,X) \\propto L(Y|X,A,\\Sigma) \\times p(\\kappa_{\\Sigma}) \\times p(A|\\Sigma,\\kappa_a) \\times p(\\Sigma|\\kappa_{\\Sigma}) \\times p(\\kappa_a) \\\\\n\\\\ \\propto p(\\kappa_{\\Sigma}) \\times p(\\Sigma|\\kappa_{\\Sigma}) \\\\\n\\\\ \\propto (\\kappa_{\\Sigma})^{-\\frac{\\underline{\\nu}N}{2}}\nexp\\left[{ -\\frac{1}{2} \\frac{\\kappa_{\\Sigma}}{tr\\left [\\Sigma^{-1}\\underline{s}_{\\Sigma} \\right ]^{-1}}} \\right\\}  \\times (\\kappa_{\\Sigma})^{\\underline{a}_{\\Sigma}-1}exp\\left [{-\\frac{\\kappa_{\\Sigma}}{\\underline{s}_{\\Sigma}}}  \\right ] \\\\\n\\\\ \\propto (\\kappa_{\\Sigma})^\\frac{{}{N\\underline{\\nu}+2\\underline{a}_{\\Sigma}-2}}{2}exp\\left [{-\\frac{\\kappa_{\\Sigma}}{\\left [2tr\\left[\\Sigma^{-1}\\underline{s}_{\\Sigma} \\right ]^{-1}+\\left[\\underline{s}_{\\Sigma} \\right ]^{-1}\\right ]^{-1}}}  \\right ] \\\\\n\\end{gather}\\] where we can recognise the kernel of an Gamma Distribution with \\[\\begin{gather}\n\\bar{S}_{\\Sigma} = {\\left [2\\left[tr\\Sigma^{-1}\\underline{s}_{\\Sigma} \\right ]^{-1}+\\left[\\underline{s}_{\\Sigma} \\right ]^{-1}\\right ]^{-1}} \\\\\n\\\\ \\bar{a}_{\\Sigma} = \\frac{N\\underline{\\nu}}{2}+\\underline{a}_{\\Sigma}\n\\end{gather}\\]\nTherefore, our new full conditional posterior distribution will be as following: The full conditional posterior of \\((A,\\Sigma)\\) is: \\[\\begin{gather}\np(A,\\Sigma|X,Y,\\kappa_a,\\kappa_{\\Sigma}) \\propto L(A,\\Sigma|Y,X) \\times p(A|\\Sigma,\\kappa_a) \\times p(\\Sigma|\\kappa_{\\Sigma}) \\\\\n\\\\ \\propto det(\\Sigma)^{-\\frac{K}{2}} exp \\left\\{-\\frac{1}{2}tr \\left[ \\Sigma^{-1}(Y-XA)^{'}(Y-XA)\\right] \\right\\} \\\\\n\\\\ \\times exp \\left\\{ -\\frac{1}{2} tr \\left[ \\Sigma^{-1}(A-\\underline{A})^{'}(\\kappa_a \\underline{V})^{-1}(A-\\underline{A}) \\right] \\right\\} \\\\\n\\\\ \\times det(\\Sigma)^{\\frac{\\underline{\\nu}+N+1}{2}} exp\\left\\{-\\frac{1}{2} tr \\left[\\Sigma^{-1}\\kappa_{\\Sigma} \\right] \\right\\}\n\\end{gather}\\]\nWe recognize kernel of matrix-normal inverse Wishart distribution, with parameters as follows: \\[\\begin{gather}\n\\bar{V} = (X'X+(\\kappa_a \\underline{V}))^{-1} \\\\\n\\\\ \\bar{A} = \\bar{V}(X'Y+(\\kappa_a \\underline{V}^{-1}\\underline{A})) \\\\ \\\\ \\bar{S} = I_N\\kappa_{\\Sigma}+Y'Y+\\underline{A}^{'}(\\kappa_a \\underline{V})^{-1}\\underline{A} - \\bar{A}^{'} \\bar{V}^{-1}\\bar{A} \\\\\n\\\\ \\bar{\\nu} = T + \\underline{\\nu}\n\\end{gather}\\]"
  },
  {
    "objectID": "index.html#artificial-data",
    "href": "index.html#artificial-data",
    "title": "A Proxy-SVAR Reproduction Model",
    "section": "Artificial Data",
    "text": "Artificial Data\nI generate two random walks \\(y1\\) and \\(y2\\) with 1000 observations each simulated from a bivariate Gaussian random walk process. I plot the raw series and the first difference below:\n\n  #artificial data with 1000 observations from a bi-variate Gaussian random walk\n  #VAR model with N=2, p=1, and a constant term\n  # simulate 1000 samples from the bivariate Gaussian random walk process with cumsum(rnorm())\n  y1 <- ts(cumsum(rnorm(1000, 0, sd=1)))\n  y2 <- ts(cumsum(rnorm(1000, 0, sd=1)))\n  y.art <- cbind(y1,y2)\n  \n  par(mfrow=c(1,2),mar=c(2,2,2,2))\n  plot(y.art, 1:1000, type = \"l\", main = \"Artificial data\", ylab=\"\", xlab=\"\")\n\n\n\n  plot(diff(y.art), 1:1000, type = \"l\", main = \"Artificial data first difference\", ylab=\"\", xlab=\"\")\n\n\n\n\nThen, I use my two functions from the basic and extended model to simulate the posterior of the intercept term. I show in the plots that for 10 thousand draws I get an intercept close to zero for the \\(A_{1,1}\\) and \\(A_{1,2}\\) values, and an identity matrix for the \\(B_{1,1}\\) and \\(B_{2,2}\\) values.\n\n  set.seed(420)\n  basic.artificial <- basic.model(y.art, 1, 10000, c(), c())\n  extended.artificial <- extended.model(y.art, 1, 1000, 9000)\n  B.basic    <- basic.artificial$B\n  A.basic    <- basic.artificial$A\n  B.extended <- extended.artificial$B.convergence\n  A.extended <-extended.artificial$A.convergence\n  \n  par(mfrow=c(4,2),mar=c(2,2,3,2))\n  plot(1:10000, B.basic[1,1,], type = \"l\", main = expression(paste(\"basic \" , B[11])), lwd=0.3, ylab=\"\", xlab=\"\", col=\"royalblue\")\n    plot(1:10000, B.basic[2,2,], type = \"l\", main = expression(paste(\"basic \" , B[22])), lwd=0.3, ylab=\"\", xlab=\"\", col=\"royalblue\")\n  plot(1:10000, A.basic[1,1,], type = \"l\", main = expression(paste(\"basic \" , A[11])), lwd=0.4, ylab=\"\", xlab=\"\", col=\"tomato\")\n  plot(1:10000, A.basic[1,2,], type = \"l\", main = expression(paste(\"basic \" , A[12])), lwd=0.4, ylab=\"\", xlab=\"\", col=\"tomato\")\n  plot(1:10000, B.extended[1,1,], type = \"l\", main = expression(paste(\"extended \" , B[11])), lwd=0.3, ylab=\"\", xlab=\"\", col=\"royalblue3\")\n  plot(1:10000, B.extended[2,2,], type = \"l\", main = expression(paste(\"extended \" , B[22])), lwd=0.3, ylab=\"\", xlab=\"\", col=\"royalblue3\")\n  plot(1:10000, A.extended[1,1,], type = \"l\", main = expression(paste(\"extended \" , A[11])), lwd=0.4, ylab=\"\", xlab=\"\", col=\"tomato3\")\n  plot(1:10000, A.extended[1,2,], type = \"l\", main = expression(paste(\"extended \" , A[12])), lwd=0.4, ylab=\"\", xlab=\"\", col=\"tomato3\")\n\n\n\n    par(mfrow=c(2,2),mar=c(2,2,3,2))\n  plot(1:10000, extended.result$B[1,1,], type = \"l\", main = expression(paste(\"basic \" , B[11])), lwd=0.3, ylab=\"\", xlab=\"\", col=\"royalblue\")\n    plot(1:10000, extended.result$B[2,2,], type = \"l\", main = expression(paste(\"basic \" , B[22])), lwd=0.3, ylab=\"\", xlab=\"\", col=\"royalblue\")\n  plot(1:10000, extended.result$A[1,1,], type = \"l\", main = expression(paste(\"basic \" , A[11])), lwd=0.4, ylab=\"\", xlab=\"\", col=\"tomato\")\n  plot(1:10000, extended.result$A[1,2,], type = \"l\", main = expression(paste(\"basic \" , A[12])), lwd=0.4, ylab=\"\", xlab=\"\", col=\"tomato\")\n\n\n\n\nMoreover, I also plot the histograms of the values \\(\\kappa_a\\) and \\(\\kappa_{\\Sigma}\\) outlining their mean value. I also add a vertical line to define the mean parameter for all the draws. The initial values for both parameters was set equal to one.\n\n  par(mfrow=c(1,2),mar=c(2,2,2,2))\n  hist(extended.artificial$k.a.convergence, breaks=\"Freedman-Diaconis\", col='tomato3', main = expression(paste(\"Slope Hyperparameter \", kappa[a])), border=F, xlim=c(0,0.5))\n  abline(v = mean(extended.artificial$k.a.convergence), col='purple4', lwd = 2)\n  hist(extended.artificial$k.sigma.convergence, breaks=\"Freedman-Diaconis\", col='royalblue', main = expression(paste(\"Covariance Hyperparameter \", kappa[Sigma])), border=F)\n  abline(v = mean(extended.artificial$k.sigma.convergence), col='purple4', lwd = 3)\n\n\n\n\n\n## plot the mean of the kappas"
  },
  {
    "objectID": "index.html#order-of-integration-of-the-variables",
    "href": "index.html#order-of-integration-of-the-variables",
    "title": "A Proxy-SVAR Reproduction Model",
    "section": "Order of Integration of the Variables",
    "text": "Order of Integration of the Variables\nI run an augmented Dickey Fuller (ADF) Test to determine the order of integration of my variables. All the variables except the instrument fail to reject the null hypotheses that the variables present a uni-root. In order to provide more insight on the order of integration I re-run an ADF test with the first difference of the variables. I expect it to be integrated of order 1 (I(1)). The ADF test of the first-difference data supports these expectations:\n\n\n\n\n\n\n\n\n\n\n\n\n\nDickey-Fuller\nLag-order\np-value\nDickey-Fuller diff\nLag-order\np-value\n\n\n\n\n-4.492\n6\n0.010\n-10.666\n6\n0.01\n\n\n-2.349\n6\n0.429\n-6.200\n6\n0.01\n\n\n-1.692\n6\n0.706\n-4.354\n6\n0.01\n\n\n-3.305\n6\n0.071\n-5.053\n6\n0.01\n\n\n-2.032\n6\n0.562\n-4.474\n6\n0.01\n\n\n-2.612\n6\n0.318\n-4.047\n6\n0.01\n\n\n-1.431\n6\n0.815\n-7.161\n6\n0.01\n\n\n-2.031\n6\n0.563\n-7.166\n6\n0.01\n\n\n-1.857\n6\n0.636\n-6.564\n6\n0.01\n\n\n-1.520\n6\n0.778\n-5.558\n6\n0.01\n\n\n-2.699\n6\n0.282\n-5.411\n6\n0.01\n\n\n-3.276\n6\n0.076\n-5.696\n6\n0.01\n\n\n-1.529\n6\n0.774\n-5.348\n6\n0.01\n\n\n-1.637\n6\n0.729\n-6.037\n6\n0.01"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "A Proxy-SVAR Reproduction Model",
    "section": "",
    "text": "::: justify > Abstract. > > Keywords. proxy svars, impulse responses, U.S. mps"
  },
  {
    "objectID": "index.html#gibbs-sampler",
    "href": "index.html#gibbs-sampler",
    "title": "A Proxy-SVAR Reproduction Model",
    "section": "Gibbs Sampler",
    "text": "Gibbs Sampler\nIn order to reach our analytical solutions we use a Monte Carlo Markov Chain (MCMC) method, namely the Gibbs Sampler procedure. We generate random draws from the joint posterior distribution and we update them at each iteration to compute our posterior distribution parameters. In our case we exploit the following procedure: Initialize \\(\\kappa_a\\) and \\(\\kappa_{\\Sigma}\\) at \\(\\kappa_a^{(0)}\\) and \\(\\kappa^{(0)}_{\\Sigma}\\)\nAt each iteration s:\n\nDraw $ (A,)^{(s)} p(A,| X,Y,*a^{(s-1)},* ^{(s-1)}) $\nDraw \\(\\kappa_a^{(s)} \\sim p(\\kappa_a|Y,X,A,\\Sigma)\\)\nDraw \\(\\kappa_{\\Sigma}^{(s)} \\sim p(\\kappa_{\\Sigma}|Y,X,A,\\Sigma)\\)\n\nRepeat steps 1 and 2 for \\((S_1 + S_2)\\) times. Discard the first \\(S_1\\) repetitions. Return the output as \\(\\left \\{ A^{(s)}, \\Sigma^{(s)} \\right \\}^{S2}_{s=S1+1}\\).\n\n  extended.model <- function(bigy, p, S1, S2){\n  ############################################################\n  \n  Y       = bigy[(p+1):nrow(bigy),]\n  X       = matrix(1,nrow(Y),1)\n  for (i in 1:p){\n    X     = cbind(X,bigy[(p+1):nrow(bigy)-i,])\n  }\n  \n  N       = ncol(bigy)\n  K       = 1+N*p\n  S=S1+S2 #sum of replications\n  # set the priors\n  A.prior     = matrix(0,K,N)\n  A.prior[2:(N+1),] <- diag(c(0,rep(1,N-1)))\n  V.prior         = (diag(c(10,1*((1:p)^(-2))%x%rep(1,N)))) #10 is kappa.2, 1 is kappa.1\n  nu.prior        = N+1\n  #kappa.a priors\n  nu.prior.a      = 3        #ig2\n  s.a.prior       = 0.01\n  #kappa.sigma priors\n  s.sigma.prior   = 0.01 #ig2\n  a.sigma.prior   = 1           #rgamma\n  \n  A.posterior         = array(rnorm(prod(c(K,N,S))), c(K, N, S))\n  B.posterior         = array(NA,c(N,N,S))\n  Sigma.post          = array(NA,c(N,N,S))\n  k.sigma             = matrix(NA, S, 1)\n  k.a                 = matrix(NA, S, 1)\n  k.sigma[1] <- 1\n  k.a[1]     <- 1\n    \n  nu.bar      = nrow(Y) + nu.prior\n  nu.bar.a    = nu.prior.a + N*K\n  a.sigma.bar = (N*nu.prior)/2+a.sigma.prior\n  \n  for (s in 1:S) {\n    # normal-inverse Wishart, IG2, and Gamma posterior parameters\n    ############################################################\n    V.bar.inv   = t(X)%*%X + (k.a[s]^-1)*diag(1/diag(V.prior))\n    V.bar       = solve(V.bar.inv)\n    A.bar       = V.bar%*%(t(X)%*%Y + diag(1/diag(k.a[s]*V.prior))%*%A.prior)\n    S.bar       = k.sigma[s]*diag(N) + t(Y)%*%Y + t(A.prior)%*%diag(1/diag(k.a[s]*V.prior))%*%A.prior - t(A.bar)%*%V.bar.inv%*%A.bar\n    S.bar.inv   = solve(S.bar)\n    # posterior draws\n    ############################################################\n    Sigma.posterior.inv = rWishart(1, df=nu.bar, Sigma=S.bar.inv)\n    Sigma.posterior     = solve(Sigma.posterior.inv[,,1])\n    # Sigma.posterior     = matrix(Sigma.posterior, N, N)\n    L                   = t(chol(V.bar))\n    cholSigma.s       = chol(Sigma.posterior)\n    A.posterior[,,s]  = A.bar + L%*%A.posterior[,,s]%*%cholSigma.s\n    \n    s.a.bar     <- s.a.prior + sum(diag( Sigma.posterior.inv[,,1] %*% t(A.posterior[,,s]-A.prior)%*% diag(1/diag(V.prior)) %*% (A.posterior[,,s]-A.prior)))\n    s.sigma.bar   <- (2*(sum(diag(Sigma.posterior.inv[,,1]))^-1)+((s.sigma.prior)^-1))^-1\n    if (s <= S){\n      #draw k.a from IG2\n      k.a[s+1]      <- s.a.bar / rchisq(1, df=nu.bar.a)\n      #draw k.sigma from gamma distribution\n      k.sigma[s+1]  <- rgamma(1, shape = a.sigma.bar, scale = s.sigma.bar)\n    }\n    \n    B.posterior[,,s]  = t(chol(Sigma.posterior))\n    Sigma.post[,,s]   = Sigma.posterior\n  }\n  #return the parameters of interest\n  results = list(\"B\" = B.posterior[,,S1+1:S2], \n                 \"A\" = A.posterior[,,S1+1:S2], \n                 \"Sigma\" = Sigma.post[,,S1+1:S2], \n                 \"k.a\" = k.a[S1+1:S2], \n                 \"k.sigma\" = k.sigma[S1+1:S2], \n                 \"B.convergence\" = B.posterior[,,1:S], \n                 \"A.convergence\" = A.posterior[,,1:S], \n                 \"Sigma.convergence\" = Sigma.post[,,1:S], \n                 \"k.a.convergence\" = k.a[1:S], \n                 \"k.sigma.convergence\" = k.sigma[1:S])\n  return(results)\n  }"
  },
  {
    "objectID": "index.html#data-description",
    "href": "index.html#data-description",
    "title": "A Proxy-SVAR Reproduction Model",
    "section": "Data Description",
    "text": "Data Description\nAgainst this backdrop, our interest is to study the degree by which U.S. monetary policy shocks can propagate throughout the economies of the world. In order to have a global picture we study macroeconomic variables that can affect and influence a FED change in the interest rate. The global variables are constructed scrutinizing these countries: Argentina, Australia, Austria, Belarus, Belgium, Bolivia, Brazil, Bulgaria, Canada, Chile, Colombia, Costa Rica, Croatia, Cyprus, Czech Republic, Denmark, Ecuador, Finland, France, Germany, Greece, Hong Kong, Hungary, Iceland, Indonesia, Ireland, Italy, Japan, Latvia, Lithuania, Luxembourg, Malaysia, Malta, Mexico, Netherlands, New Zealand, Norway, Poland, Portugal, Romania, Russia, Serbia, Singapore, Slovakia, Slovenia, South Africa, South Korea, Spain, Sweden, Switzerland, Thailand, Turkey, U.K., and the U.S.\nAs discussed in the previous section, I am ìncluding the variables of the global VAR of Miranda-Agrippino and Rey (2020). In the first graph, I include the following variables: -the U.S. industrial production index that measures the real output of all relevant establishments located in the U.S.; -the Bank for International Settlement (BIS) effective exchange rate (EER) for the United States, i.e. a summary measure calculated by the BIS to account for changes in the U.S. bilateral exchange rate against other countries by their trade importance; -global inflows, defined as direct cross-border credit flows from the U.S. to the aforementioned countries’ banks and non-banks recipients. This variable is key to explain the degree of financial dependency of the rest of the world vis-à-vis the financial hegemon; -global domestic credit, another key variable in our VAR that outlines the total amount of funds in the world economy, including loans, debt instruments, and other forms of credit provided by financial institutions.\nIn the second graph, I outline four types of leverage. First of all, leverage can be defined as the ratio between assets and equity, with equity being the difference of assets and debts. In finance, this measurement refers to the use of borrowed funds to finance assets or investments. Against this backdrop, the authors construct this variable as the ratio between claims on private sector, i.e. credit extended by banks and other financial institutions to the private sector, and the sum of transferable deposits held by depository corporations, excluding central banks. This ratio reflects the proportion of credit extended to the private sector relative to the deposits held by depository corporations, excluding central banks. A higher ratio suggests a higher level of leverage in the banking sector. The authors differentiate between different leverages given the contrasting risk-taking behavior of financial agents. As a matter of fact, we can observe that the leverage of EU global banks and US brokers and dealers is much higher than the leverage of EU and US. EU global banks include banks that are systematically important such as UBS and Unicredit. Global banks’ leverage is higher because of several reasons such as size and risk appetite. The behavior of brokers and dealers can be explained by their risk-loving approach, given that a higher total asset growth is correlated with an increase in leverage growth (insert data from Fred). On the other hand, commercial banks tend to have constant leverage growth, notwithstanding their total asset growth (insert graph).\nIn the third graph, I include the FED policy rate coupled with the global real economic activity index (excluding the US). We can observe the lagged effect of an increase in the interest rate with a decrease in global economic activity. In the fourth graph I include the global factor discussed in the previous section, plotted with the global risk aversion (one variable is the inverse of the other). The factor is rotated to provide an intuitive view that an increase in the factor (risk aversion) reflects an increase in global asset prices. Lastly, I plot the U.S. PCE deflator measuring changes in the price of goods and services over time.\nOn top of these variables, I will add the aforementioned instrument on top of the \\(y_t\\) variable. In the next section I will provide reason for this procedure."
  },
  {
    "objectID": "index.html#impulse-response-functions",
    "href": "index.html#impulse-response-functions",
    "title": "A Proxy-SVAR Reproduction Model",
    "section": "Impulse Response Functions",
    "text": "Impulse Response Functions\nMy model comprises the instrument in the first position, and I will compute impulse response functions assuming that the instrument will affect all the other variables exogenously. The assumptions of Plagborg-Moller (2021) are satisfied given our Bayesian framework and our cholesky decomposition: \\[\n\\begin{bmatrix}\nb_0^{1,1} & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\nb_0^{2,1} & b_0^{2,2} & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\nb_0^{3,1} & b_0^{3,2} & b_0^{3,3} & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\nb_0^{4,1} & b_0^{4,2} & b_0^{4,3} & b_0^{4,4} & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\nb_0^{5,1} & b_0^{5,2} & b_0^{5,3} & b_0^{5,4} & b_0^{5,5} & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\nb_0^{6,1} & b_0^{6,2} & b_0^{6,3} & b_0^{6,4} & b_0^{6,5} & b_0^{6,6} & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\nb_0^{7,1} & b_0^{7,2} & b_0^{7,3} & b_0^{7,4} & b_0^{7,5} & b_0^{7,6} & b_0^{7,7} & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\nb_0^{8,1} & b_0^{8,2} & b_0^{8,3} & b_0^{8,4} & b_0^{8,5} & b_0^{8,6} & b_0^{8,7} & b_0^{8,8} & 0 & 0 & 0 & 0 & 0 & 0 \\\\\nb_0^{9,1} & b_0^{9,2} & b_0^{9,3} & b_0^{9,4} & b_0^{9,5} & b_0^{9,6} & b_0^{9,7} & b_0^{9,8} & b_0^{9,9} & 0 & 0 & 0 & 0 & 0 \\\\\nb_0^{10,1} & b_0^{10,2} & b_0^{10,3} & b_0^{10,4} & b_0^{10,5} & b_0^{10,6} & b_0^{10,7} & b_0^{10,8} & b_0^{10,9} & b_0^{10,10} & 0 & 0 & 0 & 0 \\\\\nb_0^{11,1} & b_0^{11,2} & b_0^{11,3} & b_0^{11,4} & b_0^{11,5} & b_0^{11,6} & b_0^{11,7} & b_0^{11,8} & b_0^{11,9} & b_0^{11,10} & b_0^{11,11} & 0 & 0 & 0 \\\\\nb_0^{12,1} & b_0^{12,2} & b_0^{12,3} & b_0^{12,4} & b_0^{12,5} & b_0^{12,6} & b_0^{12,7} & b_0^{12,8} & b_0^{12,9} & b_0^{12,10} & b_0^{12,11} & b_0^{12,12} & 0 & 0 \\\\\nb_0^{13,1} & b_0^{13,2} & b_0^{13,3} & b_0^{13,4} & b_0^{13,5} & b_0^{13,6} & b_0^{13,7} & b_0^{13,8} & b_0^{13,9} & b_0^{13,10} & b_0^{13,11} & b_0^{13,12} & b_0^{13,13} & 0 \\\\\nb_0^{14,1} & b_0^{14,2} & b_0^{14,3} & b_0^{14,4} & b_0^{14,5} & b_0^{14,6} & b_0^{14,7} & b_0^{14,8} & b_0^{14,9} & b_0^{14,10} & b_0^{14,11} & b_0^{14,12} & b_0^{14,13} & b_0^{14,14} \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n\\text{instrument} \\\\\n\\text{pce} \\\\\n\\text{indpro} \\\\\n\\text{greaexus} \\\\\n\\text{glbinflows} \\\\\n\\text{dgs1} \\\\\n\\text{bis} \\\\\n\\text{globalf} \\\\\n\\text{globalra} \\\\\n\\text{glbcrexus} \\\\\n\\text{usbdlev} \\\\\n\\text{eubdlev} \\\\\n\\text{usbanksl} \\\\\n\\text{eubanksl} \\\\\n\\end{bmatrix}\n\\]\n\n  # Impulse response functions\n  # Forecast Error Variance Decomposition\n  ###########################################################\n  set.seed(420)\n  y = cbind(instrument,\n    (pce),\n    (indpro),\n    (greaexus), \n    (glbinflows),\n    dgs1, \n    bis, \n    globalf, \n    globalra, \n    (glbcrexus), \n    (usbdlev), \n    (eubdlev), \n    (usbanksl), \n    (eubanksl))\n  \n    # y = cbind(instrument,\n    # log(pce),\n    # log(indpro),\n    # greaexus, \n    # log(glbinflows),\n    # dgs1, \n    # log(bis), \n    # globalf, \n    # globalra, \n    # log(glbcredit), \n    # log(usbdlev), \n    # log(eubdlev), \n    # usbanksl, \n    # eubanksl)\n  \n  y                   <- window((y), start=c(1990,2), end=c(2010,12))\n  basic.results <- basic.model(y, 12, 10000, c(1990,2), c(2010,12))\n  N=ncol(y)\n  h=24\n  p=12\n  S=10000 #S2 draws of the extended model\n  IRF.posterior     = array(NA,c(N,N,h+1,S))\n  IRF.inf.posterior = array(NA,c(N,N,S))\n  FEVD.posterior    = array(NA,c(N,N,h+1,S))\n  J                 = cbind(diag(N),matrix(0,N,N*(p-1))) #[I_N,0_{NxN(p-1)}]\n  \n  for (s in 1:S){\n  A.bold          = rbind(t(basic.results$A[2:(1+N*p),,s]),cbind(diag(N*(p-1)),matrix(0,N*(p-1),N)))  #rbind([A1...AP], cbind(I_{N(p-1)}, 0_{N(p-1)xN}))\n  IRF.inf.posterior[,,s]          = J %*% solve(diag(N*p)-A.bold) %*% t(J) #J*(I-A)^-1*J'\n  A.bold.power    = A.bold\n  for (i in 1:(h+1)){\n    if (i==1){\n      IRF.posterior[,,i,s]        = basic.results$B[,,s] #first value is first IRF\n    } else {\n      IRF.posterior[,,i,s]        = J %*% A.bold.power %*% t(J) %*%basic.results$B[,,s] #J*A*J'*B.posterior\n      A.bold.power                = A.bold.power %*% A.bold\n    }\n    for (n in 1:N){\n    for (nn in 1:N){\n    FEVD.posterior[n,nn,i,s]  = sum(IRF.posterior[n,nn,1:i,s]^2)\n     }\n    }\n    FEVD.posterior[,,i,s]         = diag(1/apply(FEVD.posterior[,,i,s],1,sum))%*%FEVD.posterior[,,i,s]\n  }\n}\n  FEVD.posterior    = 100*FEVD.posterior\n\n  #save(IRF.posterior,IRF.inf.posterior, FEVD.posterior, file=(\"irf-instrument-fevd.RData\"))\n\n  # colours for IRFs\n  ############################################################\n  mcxs2  = \"#379683\"\n  mcxs3  = \"#5CDB95\"\n  mcxs4  = \"#8EE4AF\"\n  mcxs3.rgb   = col2rgb(mcxs3)\n  mcxs3.shade1= rgb(mcxs3.rgb[1],mcxs3.rgb[2],mcxs3.rgb[3], alpha=120, maxColorValue=255)\n  mcxs4.rgb   = col2rgb(mcxs4)\n  mcxs4.shade1= rgb(mcxs4.rgb[1],mcxs4.rgb[2],mcxs4.rgb[3], alpha=120, maxColorValue=255)\n\n\n  # plot IRFs extended model\n  ############################################################\n  #load(\"irf-instrument-fevd.RData\")\n  IRF.posterior.inst = IRF.posterior[,1,,] #takes the instruments IRFs\n  IRFs.k1           = apply(IRF.posterior.inst,1:2,median) #mean of the S IRFs\n  IRFs.inf.k1       = apply(IRF.posterior.inst,1,mean)\n  rownames(IRFs.k1) = y.names\n\n  IRFs.k1.hdi    = apply(IRF.posterior.inst,1:2,hdi, credMass=0.68)\n  IRFs.k1.hdi2    = apply(IRF.posterior.inst,1:2,hdi, credMass=0.9)\n  hh          = 1:(h+1)\n  #pdf(file=\"FF-irf-mps1.pdf\", height=9, width=12)\n  par(mfrow=c(5,2), mar=c(2, 2, 2, 1.5),cex.axis=0.8, cex.lab=0.8, cex.main=0.9)\n  IRFs.wanted      <- c(2,3,4,5,6,7,9,10,12,14)\n  for (n in 1:14){\n    if (n %in% IRFs.wanted) {\n  ylims     = range(IRFs.k1[n,hh],IRFs.k1.hdi[,n,1:4],0)\n  plot(hh,IRFs.k1[n,hh], type=\"l\", ylim=ylims, axes=FALSE, xlab=\"\", ylab=\"\", main =rownames(IRFs.k1)[n])\n  lines(hh, IRFs.k1[n,hh], lwd=2, col=mcxs2)\n  axis(1,c(4,8,12,16,20,24),c(\"4\",\"8\",\"12\",\"16\",\"20\",\"24\"))\n  axis(2,c(ylims[1],0,ylims[2]),round(c(ylims[1],0,ylims[2]),3))\n  polygon(c(hh,(h+1):1), c(IRFs.k1.hdi[1,n,hh],IRFs.k1.hdi[2,n,(h+1):1]), col=mcxs3.shade1,border=mcxs3.shade1)\n  polygon(c(hh,(h+1):1), c(IRFs.k1.hdi2[1,n,hh],IRFs.k1.hdi2[2,n,(h+1):1]), col=mcxs4.shade1,border=mcxs4.shade1)\n  abline(h=0)\n    }\n  }\n\n\n\n\n\n  # colours for FEVDs\n  ############################################################\n  colours = c(\"deepskyblue1\",\"deepskyblue2\",\"deepskyblue\",\"deepskyblue3\",\"deepskyblue4\",\"dodgerblue\",\n           \"dodgerblue1\",\"dodgerblue2\",\"maroon1\",\"maroon\",\"maroon2\",\"magenta\",\"maroon3\",\"maroon4\")\n\n  # plot FEVDs for instrument shock\n  ############################################################\n  fevd.inst  = apply(FEVD.posterior[1,,,],1:2,mean) #real gdp\n  fevd.inst  = rbind(rep(0,h+1),apply(fevd.inst,2,cumsum))\n\n  #pdf(file=\"fevd-inst.pdf\", height=7, width=12)\n  par(mar=rep(4,4),cex.axis=1, cex.lab=0.8)\n  plot(hh,fevd.inst[1,], type=\"n\", ylim=c(0,100), axes=FALSE, xlab=\"\", ylab=\"\")\n  axis(1,hh,c(\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"1 year\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"2 years\",\"\"))\n  axis(2,c(0,50,100),c(\"\",\"FEVD[inst]\",\"\"))\n  for (n in 1:N){\n   polygon(c(hh,(h+1):1), c(fevd.inst[n,hh],fevd.inst[n+1,(h+1):1]), col=colours[n],border=colours[n])\n  }  \n  axis(4, (0.5*(fevd.inst[1:14,9]+fevd.inst[2:15,9]))[c(2,4,9,10)], c(\"pce\",\"greaexus\",\"globalra\",\"glbcredit\"))\n\n\n\n  dev.off()\n\nnull device \n          1 \n\n  fevd.inst  = apply(FEVD.posterior[2,,,],1:2,mean) #real gdp\n  fevd.inst  = rbind(rep(0,h+1),apply(fevd.inst,2,cumsum))\n\n  #pdf(file=\"fevd-inst.pdf\", height=7, width=12)\n  par(mar=rep(4,4),cex.axis=1, cex.lab=0.8)\n  plot(hh,fevd.inst[1,], type=\"n\", ylim=c(0,100), axes=FALSE, xlab=\"\", ylab=\"\")\n  axis(1,hh,c(\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"1 year\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"2 years\",\"\"))\n  axis(2,c(0,50,100),c(\"\",\"FEVD[inst]\",\"\"))\n  for (n in 1:N){\n   polygon(c(hh,(h+1):1), c(fevd.inst[n,hh],fevd.inst[n+1,(h+1):1]), col=colours[n],border=colours[n])\n  }  \n  axis(4, (0.5*(fevd.inst[1:14,9]+fevd.inst[2:15,9]))[c(2,4,9,10)], c(\"pce\",\"greaexus\",\"globalra\",\"glbcredit\"))\n  dev.off()\n\nnull device \n          1"
  }
]